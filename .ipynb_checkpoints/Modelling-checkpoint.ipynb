{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335174c0-ed5d-46ee-8285-184703590a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install imbalanced-learn\n",
    "# %pip install lightgbm\n",
    "# export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\"\n",
    "# export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c96760f-b68e-401e-a2a6-d69c9e90ae53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14959506-3046-46fa-ae5a-d6f02c27c3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfda5fe-a38e-486f-bf34-893fe118d424",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802cd335-d59d-4c25-bf08-eee02baee785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>airline_name</th>\n",
       "      <th>NPS_category</th>\n",
       "      <th>NPS</th>\n",
       "      <th>language_info</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>uppercase_words</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>flesch_reading_score</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>2023-09-18 03:29:15</td>\n",
       "      <td>EasyJet sent text at 4.00 am day of flight hom...</td>\n",
       "      <td>easyjet</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>text rush organise transport effort implicatio...</td>\n",
       "      <td>194</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>5.878788</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>81.90</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-12-01 00:00:00</td>\n",
       "      <td>Its been a few years when I flew a lot in A...</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>asia vistara surprise swift immaculate steward...</td>\n",
       "      <td>608</td>\n",
       "      <td>109</td>\n",
       "      <td>74</td>\n",
       "      <td>5.527273</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>61.19</td>\n",
       "      <td>14.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>2018-09-09 20:37:11</td>\n",
       "      <td>one of the worst experiences with Air France e...</td>\n",
       "      <td>air france</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>ever bore carry duty inbound</td>\n",
       "      <td>204</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>6.580645</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>61.33</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>2019-12-23 14:00:19</td>\n",
       "      <td>Not a single star this airlines deserves .I lo...</td>\n",
       "      <td>egyptair</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>deserves last block respond mail operator harr...</td>\n",
       "      <td>464</td>\n",
       "      <td>87</td>\n",
       "      <td>64</td>\n",
       "      <td>5.272727</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>74.39</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>2023-12-29 06:42:56</td>\n",
       "      <td>I was forced to pay 150-euro worth penalty for...</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>penalty fail earth would dare season arrogant ...</td>\n",
       "      <td>240</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>6.153846</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>58.58</td>\n",
       "      <td>10.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           overall_rating          review_date  \\\n",
       "0  Rated 1 out of 5 stars  2023-09-18 03:29:15   \n",
       "1                       9  2023-12-01 00:00:00   \n",
       "3  Rated 1 out of 5 stars  2018-09-09 20:37:11   \n",
       "4  Rated 1 out of 5 stars  2019-12-23 14:00:19   \n",
       "5  Rated 1 out of 5 stars  2023-12-29 06:42:56   \n",
       "\n",
       "                                         review_text airline_name  \\\n",
       "0  EasyJet sent text at 4.00 am day of flight hom...      easyjet   \n",
       "1     Its been a few years when I flew a lot in A...      Vistara   \n",
       "3  one of the worst experiences with Air France e...   air france   \n",
       "4  Not a single star this airlines deserves .I lo...     egyptair   \n",
       "5  I was forced to pay 150-euro worth penalty for...      ryanair   \n",
       "\n",
       "  NPS_category  NPS language_info  \\\n",
       "0    Detractor   -1            en   \n",
       "1     Promoter    1            en   \n",
       "3    Detractor   -1            en   \n",
       "4    Detractor   -1            en   \n",
       "5    Detractor   -1            en   \n",
       "\n",
       "                                          clean_text  text_length  word_count  \\\n",
       "0  text rush organise transport effort implicatio...          194          32   \n",
       "1  asia vistara surprise swift immaculate steward...          608         109   \n",
       "3                       ever bore carry duty inbound          204          30   \n",
       "4  deserves last block respond mail operator harr...          464          87   \n",
       "5  penalty fail earth would dare season arrogant ...          240          38   \n",
       "\n",
       "   unique_word_count  word_density  uppercase_words  comma_count  \\\n",
       "0                 29      5.878788                0            1   \n",
       "1                 74      5.527273                3            7   \n",
       "3                 30      6.580645                0            2   \n",
       "4                 64      5.272727                4            2   \n",
       "5                 35      6.153846                1            1   \n",
       "\n",
       "   exclamation_count  question_mark_count  avg_sentence_length  \\\n",
       "0                  0                    0             6.600000   \n",
       "1                  0                    0            27.250000   \n",
       "3                  0                    0            10.000000   \n",
       "4                  0                    0            14.833333   \n",
       "5                  0                    1            12.666667   \n",
       "\n",
       "   flesch_reading_score  gunning_fog_index  \n",
       "0                 81.90               5.06  \n",
       "1                 61.19              14.08  \n",
       "3                 61.33               8.00  \n",
       "4                 74.39               7.45  \n",
       "5                 58.58              10.34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"DataScraping_and_processing/karthik_cleaned_data.csv\", index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69468b56-d349-4a8f-b215-b73a5ceab608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall_rating            0\n",
       "review_date               0\n",
       "review_text               0\n",
       "airline_name              0\n",
       "NPS_category              0\n",
       "NPS                       0\n",
       "language_info             0\n",
       "clean_text              719\n",
       "text_length               0\n",
       "word_count                0\n",
       "unique_word_count         0\n",
       "word_density              0\n",
       "uppercase_words           0\n",
       "comma_count               0\n",
       "exclamation_count         0\n",
       "question_mark_count       0\n",
       "avg_sentence_length       0\n",
       "flesch_reading_score      0\n",
       "gunning_fog_index         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549c0dc8-17f2-46fb-b6d4-d3bd7fd11b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8703de-1405-4a92-bd9d-d4e08e1fe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NPS_category\"] = df[\"NPS_category\"].map({\"Detractor\": 0, \"Neutral\": 1, \"Promoter\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947b9c9-1135-4e21-a506-f55d38a255e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d27916-cdcd-4443-8e77-8eb5512257eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94     11894\n",
      "           1       0.07      0.01      0.01       338\n",
      "           2       0.78      0.47      0.58      1920\n",
      "\n",
      "    accuracy                           0.89     14152\n",
      "   macro avg       0.58      0.48      0.51     14152\n",
      "weighted avg       0.86      0.89      0.87     14152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating bag of words representation\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
    "\n",
    "clf_bow = MultinomialNB()\n",
    "clf_bow.fit(X_train_bow, y_train_bow)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "print(\"BoW Classification Report:\\n\", classification_report(y_test_bow, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa49c5-d4f2-4d71-92d8-bcf343d839d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029a94be-101a-4235-9d84-f9826e3867a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92     11894\n",
      "           1       0.00      0.00      0.00       338\n",
      "           2       0.91      0.08      0.14      1920\n",
      "\n",
      "    accuracy                           0.85     14152\n",
      "   macro avg       0.59      0.36      0.35     14152\n",
      "weighted avg       0.84      0.85      0.79     14152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# creating a TF-IDF text representation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    tfidf_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
    "\n",
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Classification Report:\\n\", classification_report(y_test_tfidf, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7d017-9adc-4478-861e-dd481b3adcce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delta Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6319bed9-1b0c-4101-9b5a-56f896274307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1375: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     11894\n",
      "           1       1.00      0.12      0.22       338\n",
      "           2       0.98      0.88      0.93      1920\n",
      "\n",
      "    accuracy                           0.96     14152\n",
      "   macro avg       0.98      0.67      0.71     14152\n",
      "weighted avg       0.96      0.96      0.95     14152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['NPS_category'], test_size=0.25, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "class_idfs = []\n",
    "unique_classes = np.unique(y_train)\n",
    "for class_idx in unique_classes:\n",
    "    class_mask = (y_train == class_idx)\n",
    "    class_docs = X_train[class_mask]\n",
    "    class_vectorizer = TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)\n",
    "    class_tfidf = class_vectorizer.fit_transform(class_docs)\n",
    "    class_idf = class_vectorizer.idf_\n",
    "    class_idfs.append(class_idf)\n",
    "\n",
    "mean_idf = np.mean(class_idfs, axis=0)\n",
    "\n",
    "delta_idfs = [class_idf - mean_idf for class_idf in class_idfs]\n",
    "\n",
    "class_to_index = {label: index for index, label in enumerate(unique_classes)}\n",
    "\n",
    "X_train_delta_tfidf = []\n",
    "for i, doc in enumerate(X_train_tfidf):\n",
    "    class_idx = class_to_index[y_train.iloc[i]]\n",
    "    delta_idf = delta_idfs[class_idx]\n",
    "    X_train_delta_tfidf.append(doc.multiply(delta_idf))\n",
    "\n",
    "X_train_delta_tfidf = vstack(X_train_delta_tfidf)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_test_delta_tfidf = []\n",
    "for i, doc in enumerate(X_test_tfidf):\n",
    "    class_idx = class_to_index[y_test.iloc[i]]\n",
    "    delta_idf = delta_idfs[class_idx]\n",
    "    X_test_delta_tfidf.append(doc.multiply(delta_idf))\n",
    "\n",
    "X_test_delta_tfidf = vstack(X_test_delta_tfidf)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_delta_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_delta_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f6d2a-8e60-4af2-8c65-6bc465f6a7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delta Tfidf + class weights (Balancing Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf38496-7987-47ac-9de7-fb790dd0309a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     11894\n",
      "           1       0.94      0.76      0.84       338\n",
      "           2       0.84      1.00      0.92      1920\n",
      "\n",
      "    accuracy                           0.97     14152\n",
      "   macro avg       0.93      0.91      0.91     14152\n",
      "weighted avg       0.98      0.97      0.97     14152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the clean_text to create BoW representation\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42, stratify=df[\"NPS_category\"])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "\n",
    "# Initialize the classifier\n",
    "clf_bow_res = LogisticRegression(class_weight='balanced')\n",
    "# clf_bow_res = ComplementNB()\n",
    "\n",
    "# Fit the classifier on the resampled training data\n",
    "clf_bow_res.fit(X_train_delta_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set using the trained pipeline\n",
    "y_pred = clf_bow_res.predict(X_test_delta_tfidf)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2b31f-14e0-4668-ba4e-d9392eec92a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delta Tfidf + random oversampling (Balancing Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f726b11f-fc0c-4d17-b5e7-ee9c4ee600df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after Random OverSampling: Counter({0: 35651, 2: 35651, 1: 35651})\n",
      "BoW Classification Report after Random OverSampling:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     11894\n",
      "           1       0.95      0.76      0.84       338\n",
      "           2       0.87      1.00      0.93      1920\n",
      "\n",
      "    accuracy                           0.98     14152\n",
      "   macro avg       0.94      0.91      0.92     14152\n",
      "weighted avg       0.98      0.98      0.98     14152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Assuming df[\"clean_text\"] and df[\"NPS_category\"] are already defined\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the clean_text to create BoW representation\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply RandomOverSampler to training data only\n",
    "X_train_bow_res, y_train_bow_res = ros.fit_resample(X_train_delta_tfidf, y_train)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"Class distribution after Random OverSampling:\", Counter(y_train_bow_res))\n",
    "\n",
    "# Initialize the classifier\n",
    "# clf_bow_res = ComplementNB()\n",
    "clf_bow_res = LogisticRegression()\n",
    "\n",
    "# Fit the classifier on the resampled training data\n",
    "clf_bow_res.fit(X_train_bow_res, y_train_bow_res)\n",
    "\n",
    "# Predict on the original (non-resampled) testing data\n",
    "y_pred_bow_res = clf_bow_res.predict(X_test_delta_tfidf)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"BoW Classification Report after Random OverSampling:\\n\", classification_report(y_test, y_pred_bow_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928de36-2858-48a1-89cf-d50dee4f6049",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delta Tfidf + SMOTE (Balancing Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b1ccf2-9c32-4700-877d-e97697d4285b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report after SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     11894\n",
      "           1       0.54      0.93      0.68       338\n",
      "           2       0.88      0.99      0.93      1920\n",
      "\n",
      "    accuracy                           0.96     14152\n",
      "   macro avg       0.81      0.96      0.87     14152\n",
      "weighted avg       0.97      0.96      0.97     14152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Assuming df[\"clean_text\"] and df[\"NPS_category\"] are already defined\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the clean_text to create BoW representation\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "X_train_bow_smote, y_train_bow_smote = smote.fit_resample(X_train_delta_tfidf, y_train)\n",
    "\n",
    "# Initialize the classifier\n",
    "# clf_bow_smote = ComplementNB()\n",
    "clf_bow_smote = LogisticRegression()\n",
    "\n",
    "# Fit the classifier on the resampled training data\n",
    "clf_bow_smote.fit(X_train_bow_smote, y_train_bow_smote)\n",
    "\n",
    "# Predict on the original (non-resampled) testing data\n",
    "y_pred_bow_smote = clf_bow_smote.predict(X_test_delta_tfidf)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"BoW Classification Report after SMOTE:\\n\", classification_report(y_test, y_pred_bow_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb854df-555b-4243-94f4-44f8da33ea04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delta Tfidf + Adasyn (Balancing Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa51c62-bffa-4c3a-9b3e-8a54dea1b675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Classification Report after ADASYN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     11894\n",
      "           1       0.60      0.91      0.72       338\n",
      "           2       0.84      1.00      0.91      1920\n",
      "\n",
      "    accuracy                           0.96     14152\n",
      "   macro avg       0.81      0.95      0.87     14152\n",
      "weighted avg       0.97      0.96      0.96     14152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daaa/opt/miniconda3/envs/torchenv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Assuming df[\"clean_text\"] and df[\"NPS_category\"] are already defined\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the clean_text to create BoW representation\n",
    "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
    "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Apply ADASYN to training data only\n",
    "X_train_bow_adasyn, y_train_bow_adasyn = adasyn.fit_resample(X_train_delta_tfidf, y_train)\n",
    "\n",
    "# Initialize the classifier\n",
    "# clf_bow_adasyn = ComplementNB()\n",
    "clf_bow_adasyn = LogisticRegression()\n",
    "\n",
    "# Fit the classifier on the resampled training data\n",
    "clf_bow_adasyn.fit(X_train_bow_adasyn, y_train_bow_adasyn)\n",
    "\n",
    "# Predict on the original (non-resampled) testing data\n",
    "y_pred_bow_adasyn = clf_bow_adasyn.predict(X_test_delta_tfidf)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"BoW Classification Report after ADASYN:\\n\", classification_report(y_test, y_pred_bow_adasyn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54feef4a-ed17-4953-90a1-89d47aff90a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelling\n",
    "\n",
    "\n",
    "SGD instead of SVM: https://stackoverflow.com/questions/29704231/in-sklearn-what-is-the-difference-between-a-svm-model-with-linear-kernel-and-a-s <br>\n",
    "Adaboost: https://www.mygreatlearning.com/blog/adaboost-algorithm/ <br>\n",
    "Catboost: https://www.geeksforgeeks.org/catboost-ml/ <br>\n",
    "LightGBM: https://www.kdnuggets.com/2022/01/data-scientists-reasons-lightgbm.html <br>\n",
    "XGBoost: https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349 <br>\n",
    "Boosting Comparison: https://towardsdatascience.com/how-to-select-between-boosting-algorithm-e8d1b15924f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380fe469-837e-4d57-a0e2-0cb2fa36849f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight='balanced', max_iter=2000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"SVM (SGD)\": SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(enable_categorical=True, eval_metric='mlogloss'),\n",
    "    \"AdaBoost+DT\": AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100),\n",
    "    \"AdaBoost+LR\": AdaBoostClassifier(base_estimator=LogisticRegression(multi_class='multinomial',class_weight='balanced',max_iter=1000),n_estimators=100),\n",
    "    # \"Catboost\": CatBoostClassifier(iterations=500,learning_rate=0.1,depth=6,l2_leaf_reg=3,cat_features=[], auto_class_weights='Balanced',verbose=200),\n",
    "    # \"LightGBM\": LGBMClassifier(boosting_type='gbdt', objective='multiclass',  learning_rate=0.09, max_depth=-1, random_state=42, n_estimators=100, device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
    "}\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return {\"Model\": name, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e580fd87-0a92-4b99-b032-dd86aa2f29b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Models: 100%|██████████████████████████| 7/7 [00:47<00:00,  6.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.972583</td>\n",
       "      <td>0.976182</td>\n",
       "      <td>0.972583</td>\n",
       "      <td>0.973122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.975558</td>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.963672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (SGD)</td>\n",
       "      <td>0.982617</td>\n",
       "      <td>0.982797</td>\n",
       "      <td>0.982617</td>\n",
       "      <td>0.980896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.977176</td>\n",
       "      <td>0.976849</td>\n",
       "      <td>0.977176</td>\n",
       "      <td>0.976922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.970322</td>\n",
       "      <td>0.970821</td>\n",
       "      <td>0.970322</td>\n",
       "      <td>0.968711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost+DT</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.953768</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.949724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost+LR</td>\n",
       "      <td>0.898954</td>\n",
       "      <td>0.955261</td>\n",
       "      <td>0.898954</td>\n",
       "      <td>0.917976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score\n",
       "0  Logistic Regression  0.972583   0.976182  0.972583  0.973122\n",
       "1        Decision Tree  0.956190   0.975558  0.956190  0.963672\n",
       "2            SVM (SGD)  0.982617   0.982797  0.982617  0.980896\n",
       "3        Random Forest  0.977176   0.976849  0.977176  0.976922\n",
       "4              XGBoost  0.970322   0.970821  0.970322  0.968711\n",
       "5          AdaBoost+DT  0.953010   0.953768  0.953010  0.949724\n",
       "6          AdaBoost+LR  0.898954   0.955261  0.898954  0.917976"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Evaluating Models\"):\n",
    "    results_row = evaluate_model(name, model, X_train_delta_tfidf, y_train, X_test_delta_tfidf, y_test)\n",
    "    results = results._append(results_row, ignore_index=True)\n",
    "    # display(results.iloc[-1])\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ccad6-eac8-47e2-95dc-0a0a02d81403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7272c58-272d-44b3-a0ba-adedd10fdf74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
