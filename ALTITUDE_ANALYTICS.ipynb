{
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7549185,
          "sourceType": "datasetVersion",
          "datasetId": 4396626
        },
        {
          "sourceId": 7558364,
          "sourceType": "datasetVersion",
          "datasetId": 4401450
        },
        {
          "sourceId": 7558406,
          "sourceType": "datasetVersion",
          "datasetId": 4401471
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 450.193299,
      "end_time": "2024-02-04T08:20:21.18362",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-04T08:12:50.990321",
      "version": "2.5.0"
    },
    "colab": {
      "name": "ALTITUDE-ANALYTICS",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangula-karthik/Altitude-Analytics/blob/main/ALTITUDE_ANALYTICS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'training-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4396626%2F7549185%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240211%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240211T071801Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D86ac5ac0f44e23ae4c00f8dc1b74975f1c16f21f4bb22708cb8acf6f103438f012f47ad2ef24dc7d71b298a5af2676cc12c502c95f76ffd8e0395ccfae19ce32e3a7af07f66a34a1442ca2baeb29675b968ede2fd31129b45cc0d3257d82baf279e910f60721d8333318600115c691e984827c2ffdde92e7d67f23a530f3611896bd7c316fb96e2514f161b5455536dfaaa56080e520c948bbf5219b2332d43b1b88de266333b6e864d1f2bbb437ea6cb37c1f7ba92fd8a8d7a00d7da054f68161eb38787d634c71896f2ef8fa9bc8e381c0e5644ed08e5683b180e98c908bc8439f11dafaf96c44773e0d817dc11bdedfc821a3b7f694282fdefbf227adc385,testing-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4401450%2F7558364%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240211%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240211T071801Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1d11ab953466783519353f1ec18e0e2b96e57740e8b53e8b588418864a690752d31df9b4a015089448f8c64d127ea4f49334dbe25a23450f3a940441af5e80f3054b89fe79100234de936e3afe1e974d1ede1d7eb46ffb2a14b39b7e0788264d969ce4c4572c83a847dccc163713357dd8663089881104d6acee8dd2290f3eae1d264d473f07c0f40ea1083901946a7f73173f296a60a09293e10262f354059a58e6bc8d8ac188f27bd7298ad83890cc4abc308d614fa2c84359fcd99f86c79e7e12f5fae9bc6964132b74f539c89ff6fdfbe5319bda0f1a213f68d06899d6b260f55f5db827cd5dac7df8370ec9d9b3fcccfc3783b8338a3cafd050f6301288,dataprocessing:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4401471%2F7558406%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240211%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240211T071801Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Daca257a4ff2619306ae1f3090e5cd238ee2ebd302093af305ac508c74d67aace667ebd765be6db7e0149ceb949fdf6d1cc12dafb9af7ce4f80101421f0aed2c64810791c5cdb52d78734bf65bcff1f5c66b544715b9a0a246978068893b3717f85c7ffcda75c0d48b0cf8e5d8366632687a73cf6048a3b803a200fb7a462e4c40e6e553c2e1a1f46f660994d5cbe6807d193fdb7bb7f863e42cfe5cacca3a3b35833b44c2c86019532088abb04852f1f4961e1094ec2a7fffb34047cb64de386cb808c3d23ef5f6b6ef36b8dee599710db85dad0d654c813a71fc7cdc994c4e7ccd85d45e97f8ccad76a3de3e2032c74f1c9955f439476f81ec62341f46d61ca'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KxDDPWW8CybE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NPS SCORE CLASSIFICATION ON AIRLINES REVIEW TEXT"
      ],
      "metadata": {
        "id": "a4PpFiYeCybF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing Packages"
      ],
      "metadata": {
        "tags": [],
        "id": "_erCDUW3CybG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # checking for GPU ver."
      ],
      "metadata": {
        "papermill": {
          "duration": 1.079056,
          "end_time": "2024-02-04T08:12:56.002806",
          "exception": false,
          "start_time": "2024-02-04T08:12:54.92375",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:33:02.208432Z",
          "iopub.execute_input": "2024-02-11T04:33:02.208859Z",
          "iopub.status.idle": "2024-02-11T04:33:03.406006Z",
          "shell.execute_reply.started": "2024-02-11T04:33:02.208812Z",
          "shell.execute_reply": "2024-02-11T04:33:03.404335Z"
        },
        "trusted": true,
        "id": "ZSP74NI0CybG",
        "outputId": "a8e79aec-4bc9-44e7-cd17-c6cfdb2a48a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/bin/bash: nvidia-smi: command not found\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install imbalanced-learn\n",
        "# %pip install lightgbm\n",
        "# %pip install cuml\n",
        "%pip install contractions\n",
        "%pip install symspellpy\n",
        "%pip install textstat\n",
        "%pip install -U kaleido\n",
        "# %pip show cuml\n",
        "\n",
        "# %pip install gensim==3.4.0\n",
        "# %pip install smart_open==1.9.0\n",
        "\n",
        "import nltk\n",
        "# nltk.download('wordnet', '/usr/share/nltk_data')\n",
        "# nltk.download()\n",
        "# !unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01614,
          "end_time": "2024-02-04T08:12:56.027779",
          "exception": false,
          "start_time": "2024-02-04T08:12:56.011639",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:42:20.748829Z",
          "iopub.execute_input": "2024-02-11T04:42:20.750025Z",
          "iopub.status.idle": "2024-02-11T04:43:52.16458Z",
          "shell.execute_reply.started": "2024-02-11T04:42:20.749983Z",
          "shell.execute_reply": "2024-02-11T04:43:52.162814Z"
        },
        "trusted": true,
        "id": "3v52NO2ICybG",
        "outputId": "86b16d00-251b-4f51-9a4c-7149bc7d9b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\nNote: you may need to restart the kernel to use updated packages.\nCollecting symspellpy\n  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy)\n  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: editdistpy\n  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=50410 sha256=89ebee0f0f8395b5c29a99f58f00658efa770a5cf62b77aeb09a18a02e0f482a\n  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\nSuccessfully built editdistpy\nInstalling collected packages: editdistpy, symspellpy\nSuccessfully installed editdistpy-0.1.3 symspellpy-6.7.7\nNote: you may need to restart the kernel to use updated packages.\nCollecting textstat\n  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyphen (from textstat)\n  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, textstat\nSuccessfully installed pyphen-0.14.0 textstat-0.7.3\nNote: you may need to restart the kernel to use updated packages.\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import warnings\n",
        "import re\n",
        "import sys\n",
        "sys.path.append('/kaggle/input/dataprocessing/')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "from symspellpy import SymSpell\n",
        "from textstat import textstat\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                             confusion_matrix, f1_score,\n",
        "                             precision_recall_fscore_support,\n",
        "                             precision_score, recall_score)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import loguniform\n",
        "from tqdm import tqdm\n",
        "from wordcloud import WordCloud\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import contractions\n",
        "from scipy.stats import loguniform, uniform\n",
        "from scipy.sparse import hstack, vstack\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from emojii_and_emoticon_map import EMOTICONS_EMO, EMOJI_UNICODE\n",
        "# from DataScraping_and_processing.emojii_and_emoticon_map import EMOTICONS_EMO, EMOJI_UNICODE\n",
        "import langid"
      ],
      "metadata": {
        "papermill": {
          "duration": 10.570325,
          "end_time": "2024-02-04T08:13:06.606813",
          "exception": false,
          "start_time": "2024-02-04T08:12:56.036488",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:50:05.933924Z",
          "iopub.execute_input": "2024-02-11T04:50:05.934472Z",
          "iopub.status.idle": "2024-02-11T04:50:09.219031Z",
          "shell.execute_reply.started": "2024-02-11T04:50:05.934427Z",
          "shell.execute_reply": "2024-02-11T04:50:09.217185Z"
        },
        "trusted": true,
        "id": "dYDwHRFyCybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import FastText"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T04:50:09.222446Z",
          "iopub.execute_input": "2024-02-11T04:50:09.222911Z",
          "iopub.status.idle": "2024-02-11T04:50:37.830853Z",
          "shell.execute_reply.started": "2024-02-11T04:50:09.222867Z",
          "shell.execute_reply": "2024-02-11T04:50:37.829277Z"
        },
        "trusted": true,
        "id": "CM-hsqmyCybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015465,
          "end_time": "2024-02-04T08:13:06.631026",
          "exception": false,
          "start_time": "2024-02-04T08:13:06.615561",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:50:37.832876Z",
          "iopub.execute_input": "2024-02-11T04:50:37.833826Z",
          "iopub.status.idle": "2024-02-11T04:50:37.839911Z",
          "shell.execute_reply.started": "2024-02-11T04:50:37.833787Z",
          "shell.execute_reply": "2024-02-11T04:50:37.838608Z"
        },
        "trusted": true,
        "id": "4laba-6TCybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation + Feature Engineering"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008154,
          "end_time": "2024-02-04T08:13:06.647525",
          "exception": false,
          "start_time": "2024-02-04T08:13:06.639371",
          "status": "completed"
        },
        "tags": [],
        "id": "ddD9IWUzCybH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Data Cleaning\n",
        "- reading the csv file\n",
        "- removing null values (appears after data cleaning)\n",
        "- Label encoding on NPS_category `[mapping: {\"Detractor\": 0, \"Neutral\": 1, \"Promoter\": 2}]`"
      ],
      "metadata": {
        "tags": [],
        "id": "x-DrqiziCybH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/training-data/karthik_cleaned_data.csv\", index_col=[0])\n",
        "# df = pd.read_csv(\"./DataScraping_and_processing/karthik_cleaned_data.csv\", index_col=[0])\n",
        "df.head()"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.592066,
          "end_time": "2024-02-04T08:13:08.247683",
          "exception": false,
          "start_time": "2024-02-04T08:13:06.655617",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:51:40.214628Z",
          "iopub.execute_input": "2024-02-11T04:51:40.215017Z",
          "iopub.status.idle": "2024-02-11T04:51:42.008599Z",
          "shell.execute_reply.started": "2024-02-11T04:51:40.214987Z",
          "shell.execute_reply": "2024-02-11T04:51:42.00696Z"
        },
        "trusted": true,
        "id": "n6gPSahFCybH",
        "outputId": "ad354b15-7374-4fe0-a9a5-b5335d0003d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "           overall_rating          review_date  \\\n0  Rated 1 out of 5 stars  2023-09-18 03:29:15   \n1                       9  2023-12-01 00:00:00   \n3  Rated 1 out of 5 stars  2018-09-09 20:37:11   \n4  Rated 1 out of 5 stars  2019-12-23 14:00:19   \n5  Rated 1 out of 5 stars  2023-12-29 06:42:56   \n\n                                         review_text airline_name  \\\n0  EasyJet sent text at 4.00 am day of flight hom...      easyjet   \n1     Its been a few years when I flew a lot in A...      Vistara   \n3  one of the worst experiences with Air France e...   air france   \n4  Not a single star this airlines deserves .I lo...     egyptair   \n5  I was forced to pay 150-euro worth penalty for...      ryanair   \n\n  NPS_category  NPS language_info  \\\n0    Detractor   -1            en   \n1     Promoter    1            en   \n3    Detractor   -1            en   \n4    Detractor   -1            en   \n5    Detractor   -1            en   \n\n                                          clean_text  text_length  word_count  \\\n0  text rush organise transport effort implicatio...          194          32   \n1  asia vistara surprise swift immaculate steward...          608         109   \n3                       ever bore carry duty inbound          204          30   \n4  deserves last block respond mail operator harr...          464          87   \n5  penalty fail earth would dare season arrogant ...          240          38   \n\n   unique_word_count  word_density  uppercase_words  comma_count  \\\n0                 29      5.878788                0            1   \n1                 74      5.527273                3            7   \n3                 30      6.580645                0            2   \n4                 64      5.272727                4            2   \n5                 35      6.153846                1            1   \n\n   exclamation_count  question_mark_count  avg_sentence_length  \\\n0                  0                    0             6.600000   \n1                  0                    0            27.250000   \n3                  0                    0            10.000000   \n4                  0                    0            14.833333   \n5                  0                    1            12.666667   \n\n   flesch_reading_score  gunning_fog_index  \n0                 81.90               5.06  \n1                 61.19              14.08  \n3                 61.33               8.00  \n4                 74.39               7.45  \n5                 58.58              10.34  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall_rating</th>\n      <th>review_date</th>\n      <th>review_text</th>\n      <th>airline_name</th>\n      <th>NPS_category</th>\n      <th>NPS</th>\n      <th>language_info</th>\n      <th>clean_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>word_density</th>\n      <th>uppercase_words</th>\n      <th>comma_count</th>\n      <th>exclamation_count</th>\n      <th>question_mark_count</th>\n      <th>avg_sentence_length</th>\n      <th>flesch_reading_score</th>\n      <th>gunning_fog_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Rated 1 out of 5 stars</td>\n      <td>2023-09-18 03:29:15</td>\n      <td>EasyJet sent text at 4.00 am day of flight hom...</td>\n      <td>easyjet</td>\n      <td>Detractor</td>\n      <td>-1</td>\n      <td>en</td>\n      <td>text rush organise transport effort implicatio...</td>\n      <td>194</td>\n      <td>32</td>\n      <td>29</td>\n      <td>5.878788</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6.600000</td>\n      <td>81.90</td>\n      <td>5.06</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>2023-12-01 00:00:00</td>\n      <td>Its been a few years when I flew a lot in A...</td>\n      <td>Vistara</td>\n      <td>Promoter</td>\n      <td>1</td>\n      <td>en</td>\n      <td>asia vistara surprise swift immaculate steward...</td>\n      <td>608</td>\n      <td>109</td>\n      <td>74</td>\n      <td>5.527273</td>\n      <td>3</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27.250000</td>\n      <td>61.19</td>\n      <td>14.08</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rated 1 out of 5 stars</td>\n      <td>2018-09-09 20:37:11</td>\n      <td>one of the worst experiences with Air France e...</td>\n      <td>air france</td>\n      <td>Detractor</td>\n      <td>-1</td>\n      <td>en</td>\n      <td>ever bore carry duty inbound</td>\n      <td>204</td>\n      <td>30</td>\n      <td>30</td>\n      <td>6.580645</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.000000</td>\n      <td>61.33</td>\n      <td>8.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Rated 1 out of 5 stars</td>\n      <td>2019-12-23 14:00:19</td>\n      <td>Not a single star this airlines deserves .I lo...</td>\n      <td>egyptair</td>\n      <td>Detractor</td>\n      <td>-1</td>\n      <td>en</td>\n      <td>deserves last block respond mail operator harr...</td>\n      <td>464</td>\n      <td>87</td>\n      <td>64</td>\n      <td>5.272727</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14.833333</td>\n      <td>74.39</td>\n      <td>7.45</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Rated 1 out of 5 stars</td>\n      <td>2023-12-29 06:42:56</td>\n      <td>I was forced to pay 150-euro worth penalty for...</td>\n      <td>ryanair</td>\n      <td>Detractor</td>\n      <td>-1</td>\n      <td>en</td>\n      <td>penalty fail earth would dare season arrogant ...</td>\n      <td>240</td>\n      <td>38</td>\n      <td>35</td>\n      <td>6.153846</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12.666667</td>\n      <td>58.58</td>\n      <td>10.34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.057465,
          "end_time": "2024-02-04T08:13:08.315091",
          "exception": false,
          "start_time": "2024-02-04T08:13:08.257626",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T04:51:42.010574Z",
          "iopub.execute_input": "2024-02-11T04:51:42.010942Z",
          "iopub.status.idle": "2024-02-11T04:51:42.088838Z",
          "shell.execute_reply.started": "2024-02-11T04:51:42.010902Z",
          "shell.execute_reply": "2024-02-11T04:51:42.087501Z"
        },
        "trusted": true,
        "id": "TDwMgd4OCybH",
        "outputId": "42b84579-93f7-4c2d-aa95-3894822f8a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "overall_rating            0\nreview_date               0\nreview_text               0\nairline_name              0\nNPS_category              0\nNPS                       0\nlanguage_info             0\nclean_text              719\ntext_length               0\nword_count                0\nunique_word_count         0\nword_density              0\nuppercase_words           0\ncomma_count               0\nexclamation_count         0\nquestion_mark_count       0\navg_sentence_length       0\nflesch_reading_score      0\ngunning_fog_index         0\ndtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "    There are null values for clean text. This is because of the remove frequent words and rare terms function which has removed all the words from certain reviews. Need to further adjust the term frequency threshold to prevent removal of many words. These functions must be updated. The list of stopwords have also been udpated to include words like \"u\", \"never\" and \"could\" which appeared quite often when doing topic modelling.\n",
        "</div>"
      ],
      "metadata": {
        "id": "YGrbRTlpCybH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dict = {}\n",
        "word_freq = Counter()\n",
        "\n",
        "with open('/kaggle/input/dataprocessing/slang.txt', 'r') as file:\n",
        "    slang_dict = {key.lower(): value.lower() for line in file if '=' in line for key, value in [line.strip().split('=', 1)]}\n",
        "\n",
        "# with open('DataScraping_and_processing/slang.txt', 'r') as file:\n",
        "#     slang_dict = {key.lower(): value.lower() for line in file if '=' in line for key, value in [line.strip().split('=', 1)]}\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review_text'])\n",
        "\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "tfidf_sum = np.sum(tfidf_matrix, axis=0)\n",
        "tfidf_scores = np.squeeze(np.asarray(tfidf_sum))\n",
        "\n",
        "sorted_indices = np.argsort(tfidf_scores)\n",
        "\n",
        "freq_word_threshold = np.quantile(tfidf_scores, 0.99)\n",
        "rare_word_threshold = np.quantile(tfidf_scores, 0.01)\n",
        "\n",
        "FREQWORDS = set(feature_names[tfidf_scores > freq_word_threshold])\n",
        "RAREWORDS = set(feature_names[tfidf_scores < rare_word_threshold])\n",
        "\n",
        "print(\"Frequent Words:\", list(FREQWORDS)[:5])\n",
        "print(\"\\nRare Words:\", list(RAREWORDS)[:5])\n",
        "\n",
        "def detect_language(text):\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang\n",
        "\n",
        "tqdm.pandas(desc=\"Detecting languages\")\n",
        "df['language_info'] = df['review_text'].progress_apply(detect_language)\n",
        "\n",
        "def remove_rows(dataframe):\n",
        "    print(\"rows removed\")\n",
        "    return (dataframe.dropna()\n",
        "            .drop_duplicates()\n",
        "            .query(\"language_info == 'en'\")\n",
        "            .loc[dataframe['review_text'].str.split().str.len() > 2])\n",
        "\n",
        "data_cleaning_pipeline = Pipeline([\n",
        "    ('remove_rows', FunctionTransformer(remove_rows)),\n",
        "])\n",
        "\n",
        "# text cleaning functions\n",
        "\n",
        "def remove_excess_space(text):\n",
        "    text = \" \".join(text.split())\n",
        "    text = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def remove_url(text):\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    return re.sub(regex, '', text)\n",
        "\n",
        "def html_tag_remover(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def normalize_exaggerated_text(text):\n",
        "    normalized_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "    return normalized_text\n",
        "\n",
        "def replace_slang(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in slang_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: slang_dict[x.group().lower()], text)\n",
        "\n",
        "def convert_emoticons_and_emojis(text, emoticons_emo=EMOTICONS_EMO, emoji_unicode=EMOJI_UNICODE):\n",
        "    emoticon_patterns = {re.escape(k): \"_\".join(v.replace(\",\", \"\").split()) for k, v in emoticons_emo.items()}\n",
        "    emoji_patterns = {re.escape(v): k.strip(\":\") for k, v in emoji_unicode.items()}\n",
        "    combined_patterns = {**emoticon_patterns, **emoji_patterns}\n",
        "\n",
        "    pattern = re.compile('|'.join(combined_patterns.keys()))\n",
        "\n",
        "    def replace(match):\n",
        "        return combined_patterns[re.escape(match.group(0))]\n",
        "\n",
        "    return pattern.sub(replace, text)\n",
        "\n",
        "def remove_username(text):\n",
        "    return re.sub('@[^\\s]+','',text)\n",
        "\n",
        "symsp = SymSpell()\n",
        "symsp.load_dictionary('/kaggle/input/dataprocessing/my_frequency_dictionary.txt', term_index=0, count_index=1, separator=' ')\n",
        "\n",
        "def correct_spelling(text):\n",
        "    corrected = symsp.lookup_compound(text, max_edit_distance=2)\n",
        "    return corrected[0].term if corrected else text\n",
        "\n",
        "def remove_money(text):\n",
        "    pattern = r'\\b(?:\\$|\\£|\\€|\\₹)?\\w*\\d\\w*\\b'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "def fix_negation(sentence):\n",
        "    for i in range(1, len(sentence)):\n",
        "        if sentence[i-1] in ['not', \"n't\"]:\n",
        "            synsets = wordnet.synsets(sentence[i])\n",
        "            if synsets:\n",
        "                w1 = synsets[0]\n",
        "                antonyms = [l.antonyms()[0].name() for l in w1.lemmas() if l.antonyms()]\n",
        "                if antonyms:\n",
        "                    max_dissimilarity = 0\n",
        "                    for ant in antonyms:\n",
        "                        ant_synsets = wordnet.synsets(ant)\n",
        "                        if ant_synsets:\n",
        "                            similarity = w1.wup_similarity(ant_synsets[0])\n",
        "                            # Check if similarity is None and handle it\n",
        "                            if similarity is None:\n",
        "                                continue  # Skip this comparison\n",
        "                            dissimilarity = 1 - similarity\n",
        "                            if dissimilarity > max_dissimilarity:\n",
        "                                max_dissimilarity = dissimilarity\n",
        "                                sentence[i] = ant\n",
        "                    sentence[i-1] = ''\n",
        "\n",
        "    sentence = [word for word in sentence if word]\n",
        "    return ' '.join(sentence)  # Return a string instead of a list\n",
        "\n",
        "\n",
        "STOPWORDS = stopwords.words(\"english\")\n",
        "STOPWORDS += ['u', 'us', 'back', 'one', 'never', 'could']\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "def remove_freqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "\n",
        "def remove_rarewords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "text_preprocessing_pipeline = Pipeline(steps=[\n",
        "    ('lowercase', FunctionTransformer(lambda x: x.progress_apply(str.lower))),\n",
        "    ('remove excess whitespace', FunctionTransformer(lambda x: x.progress_apply(remove_excess_space))),\n",
        "    ('remove_url', FunctionTransformer(lambda x: x.progress_apply(remove_url))),\n",
        "    ('html_tag_remover', FunctionTransformer(lambda x: x.progress_apply(html_tag_remover))),\n",
        "    ('money_remover', FunctionTransformer(lambda x: x.progress_apply(remove_money))),\n",
        "    ('convert_emoticons_and_emojii', FunctionTransformer(lambda x: x.progress_apply(convert_emoticons_and_emojis))),\n",
        "    ('remove_username', FunctionTransformer(lambda x: x.progress_apply(remove_username))),\n",
        "    ('replace_slang', FunctionTransformer(lambda x: x.progress_apply(replace_slang))),\n",
        "    ('correct_spelling', FunctionTransformer(lambda x: x.progress_apply(correct_spelling))),\n",
        "    ('expand_contractions', FunctionTransformer(lambda x: x.progress_apply(lambda text: contractions.fix(text)))),\n",
        "    ('fix_negation', FunctionTransformer(lambda x: x.progress_apply(lambda text: \"\".join(fix_negation(word_tokenize(text)))))),\n",
        "    ('remove_punctuation', FunctionTransformer(lambda x: x.progress_apply(remove_punctuation))),\n",
        "    ('normalize_exaggerated_text', FunctionTransformer(lambda x: x.progress_apply(normalize_exaggerated_text))),\n",
        "    ('remove_stopwords', FunctionTransformer(lambda x: x.progress_apply(remove_stopwords))),\n",
        "    ('remove_freqwords', FunctionTransformer(lambda x: x.progress_apply(remove_freqwords))),\n",
        "    ('remove_rarewords', FunctionTransformer(lambda x: x.progress_apply(remove_rarewords))),\n",
        "    ('lemmatize_words', FunctionTransformer(lambda x: x.progress_apply(lemmatize_words)))\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T04:51:42.091321Z",
          "iopub.execute_input": "2024-02-11T04:51:42.091708Z",
          "iopub.status.idle": "2024-02-11T04:56:29.99939Z",
          "shell.execute_reply.started": "2024-02-11T04:51:42.091677Z",
          "shell.execute_reply": "2024-02-11T04:56:29.998041Z"
        },
        "trusted": true,
        "id": "pTOwOGpQCybH",
        "outputId": "459593dd-64f7-4cb9-a4a4-f9dbcbad0d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Frequent Words: ['cheap', 'poor', 'right', 'plane', 'half']\n\nRare Words: ['forsøg', 'sliema', 'automized', '35000', 'medida']\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Detecting languages: 100%|██████████| 57325/57325 [04:36<00:00, 207.27it/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = data_cleaning_pipeline.fit_transform(df)\n",
        "df['clean_text'] = text_preprocessing_pipeline.fit_transform(df['review_text'])\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T04:56:30.00164Z",
          "iopub.execute_input": "2024-02-11T04:56:30.001995Z",
          "iopub.status.idle": "2024-02-11T05:37:36.523204Z",
          "shell.execute_reply.started": "2024-02-11T04:56:30.001965Z",
          "shell.execute_reply": "2024-02-11T05:37:36.518734Z"
        },
        "trusted": true,
        "id": "cB2HunQ9CybI",
        "outputId": "3636eb55-f9d0-40bc-9fef-84ce7631c981"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "rows removed\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 56606/56606 [00:00<00:00, 182588.38it/s]\n100%|██████████| 56606/56606 [00:02<00:00, 21013.28it/s]\n100%|██████████| 56606/56606 [00:05<00:00, 10141.21it/s]\n100%|██████████| 56606/56606 [00:03<00:00, 14282.22it/s]\n100%|██████████| 56606/56606 [00:05<00:00, 11047.83it/s]\n100%|██████████| 56606/56606 [10:02<00:00, 93.88it/s] \n100%|██████████| 56606/56606 [00:00<00:00, 236918.61it/s]\n100%|██████████| 56606/56606 [00:09<00:00, 6265.20it/s]\n100%|██████████| 56606/56606 [30:27<00:00, 30.97it/s]  \n100%|██████████| 56606/56606 [00:04<00:00, 12238.30it/s]\n  0%|          | 1/56606 [00:00<32:30, 29.02it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m data_cleaning_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(df)\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtext_preprocessing_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:437\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 437\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_function_transformer.py:238\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_function_transformer.py:310\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[0;34m(self, X, func, kw_args)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 158\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, wordnet_map\u001b[38;5;241m.\u001b[39mget(pos[\u001b[38;5;241m0\u001b[39m], wordnet\u001b[38;5;241m.\u001b[39mNOUN)) \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_tagged_text])\n\u001b[1;32m    145\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m    147\u001b[0m text_preprocessing_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    148\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mlower))),\n\u001b[1;32m    149\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove excess whitespace\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_excess_space))),\n\u001b[1;32m    150\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_url\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_url))),\n\u001b[1;32m    151\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml_tag_remover\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(html_tag_remover))),\n\u001b[1;32m    152\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney_remover\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_money))),\n\u001b[1;32m    153\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_emoticons_and_emojii\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(convert_emoticons_and_emojis))),\n\u001b[1;32m    154\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_username\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_username))),\n\u001b[1;32m    155\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace_slang\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(replace_slang))),\n\u001b[1;32m    156\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect_spelling\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(correct_spelling))),\n\u001b[1;32m    157\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand_contractions\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: contractions\u001b[38;5;241m.\u001b[39mfix(text)))),\n\u001b[0;32m--> 158\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix_negation\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfix_negation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)),\n\u001b[1;32m    159\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_punctuation\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_punctuation))),\n\u001b[1;32m    160\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_exaggerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(normalize_exaggerated_text))),\n\u001b[1;32m    161\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_stopwords))),\n\u001b[1;32m    162\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_freqwords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_freqwords))),\n\u001b[1;32m    163\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_rarewords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_rarewords))),\n\u001b[1;32m    164\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatize_words\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(lemmatize_words)))\n\u001b[1;32m    165\u001b[0m ])\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:920\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
            "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:915\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 158\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, wordnet_map\u001b[38;5;241m.\u001b[39mget(pos[\u001b[38;5;241m0\u001b[39m], wordnet\u001b[38;5;241m.\u001b[39mNOUN)) \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_tagged_text])\n\u001b[1;32m    145\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m    147\u001b[0m text_preprocessing_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    148\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mlower))),\n\u001b[1;32m    149\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove excess whitespace\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_excess_space))),\n\u001b[1;32m    150\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_url\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_url))),\n\u001b[1;32m    151\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml_tag_remover\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(html_tag_remover))),\n\u001b[1;32m    152\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney_remover\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_money))),\n\u001b[1;32m    153\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_emoticons_and_emojii\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(convert_emoticons_and_emojis))),\n\u001b[1;32m    154\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_username\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_username))),\n\u001b[1;32m    155\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace_slang\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(replace_slang))),\n\u001b[1;32m    156\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect_spelling\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(correct_spelling))),\n\u001b[1;32m    157\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand_contractions\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: contractions\u001b[38;5;241m.\u001b[39mfix(text)))),\n\u001b[0;32m--> 158\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix_negation\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mfix_negation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)))),\n\u001b[1;32m    159\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_punctuation\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_punctuation))),\n\u001b[1;32m    160\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_exaggerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(normalize_exaggerated_text))),\n\u001b[1;32m    161\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_stopwords))),\n\u001b[1;32m    162\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_freqwords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_freqwords))),\n\u001b[1;32m    163\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_rarewords\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(remove_rarewords))),\n\u001b[1;32m    164\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatize_words\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mprogress_apply(lemmatize_words)))\n\u001b[1;32m    165\u001b[0m ])\n",
            "Cell \u001b[0;32mIn[15], line 103\u001b[0m, in \u001b[0;36mfix_negation\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m         synsets \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(sentence[i])\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m synsets:\n\u001b[1;32m    105\u001b[0m             w1 \u001b[38;5;241m=\u001b[39m synsets[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
          ],
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NPS_category\"] = df[\"NPS_category\"].map({\"Detractor\": 0, \"Neutral\": 1, \"Promoter\": 2})\n",
        "df.dropna(inplace=True)\n",
        "df.to_csv(\"cleaned_dataset_v2.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.538117Z",
          "iopub.status.idle": "2024-02-11T05:37:36.538978Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.538722Z",
          "shell.execute_reply": "2024-02-11T05:37:36.538745Z"
        },
        "trusted": true,
        "id": "LkU-ZpQpCybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.540281Z",
          "iopub.status.idle": "2024-02-11T05:37:36.540983Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.540768Z",
          "shell.execute_reply": "2024-02-11T05:37:36.540787Z"
        },
        "trusted": true,
        "id": "SiPy_Y7ICybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Meta text features\n",
        "- meta text features are those additional features about the text that can be used to give more information about it. These can include things like word count, count of nouns and verbs, text readability score, etc"
      ],
      "metadata": {
        "tags": [],
        "id": "lX05ycdpCybI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['text_length', 'word_count', 'unique_word_count', 'word_density', 'uppercase_words', 'comma_count', 'exclamation_count', 'question_mark_count', 'avg_sentence_length', 'flesch_reading_score', 'gunning_fog_index']]\n",
        "y = df['NPS_category']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_valid)\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "precision = precision_score(y_valid, y_pred, average='weighted')\n",
        "recall = recall_score(y_valid, y_pred, average='weighted')\n",
        "f1 = f1_score(y_valid, y_pred, average='weighted')\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision (Weighted):\", precision)\n",
        "print(\"Recall (Weighted):\", recall)\n",
        "print(\"F1-score (Weighted):\", f1)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Features': X.columns, 'Importance': feature_importance})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance_df, x='Importance', y='Features', orient=\"h\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.542163Z",
          "iopub.status.idle": "2024-02-11T05:37:36.542573Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.542384Z",
          "shell.execute_reply": "2024-02-11T05:37:36.5424Z"
        },
        "trusted": true,
        "id": "VdId3paDCybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "    The features created are not very strong at predicting the target. Upon further research some more stronger features included the noun count and the verb count.\n",
        "</div>"
      ],
      "metadata": {
        "id": "JTPU-9d9CybI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the nouns and verbs\n",
        "\n",
        "def pos_proportions(text, pos_tag):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "    tagged_sentences = pos_tag_sents(tokenized_sentences)\n",
        "\n",
        "    pos_counts = 0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in tagged_sentences:\n",
        "        for word, tag in sentence:\n",
        "            if word.isalpha():\n",
        "                total_words += 1\n",
        "                if tag.startswith(pos_tag):\n",
        "                    pos_counts += 1\n",
        "\n",
        "    return pos_counts / total_words if total_words > 0 else 0\n",
        "\n",
        "tqdm.pandas(desc=\"Counting the nouns\")\n",
        "df[\"noun_proportions\"] = df['review_text'].progress_apply(lambda x: pos_proportions(x, \"NN\"))\n",
        "tqdm.pandas(desc=\"Counting the verbs\")\n",
        "df[\"verb_proportions\"] = df['review_text'].progress_apply(lambda x: pos_proportions(x, \"VB\"))\n",
        "df[[\"clean_text\", \"noun_proportions\", \"verb_proportions\"]]"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.545208Z",
          "iopub.status.idle": "2024-02-11T05:37:36.546129Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.545869Z",
          "shell.execute_reply": "2024-02-11T05:37:36.545913Z"
        },
        "trusted": true,
        "id": "y5DbTsvzCybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.assign(noun_proportions=df['noun_proportions'], verb_proportions=df['verb_proportions'])\n",
        "y = df['NPS_category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision (Weighted):\", precision)\n",
        "print(\"Recall (Weighted):\", recall)\n",
        "print(\"F1-score (Weighted):\", f1)\n",
        "\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Features': X.columns, 'Importance': feature_importance})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance_df, x='Importance', y='Features', orient=\"h\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.547636Z",
          "iopub.status.idle": "2024-02-11T05:37:36.548313Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.548082Z",
          "shell.execute_reply": "2024-02-11T05:37:36.548102Z"
        },
        "trusted": true,
        "id": "QIskB-ZICybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.06 # threshold for removing features\n",
        "\n",
        "feature_importance_df = pd.DataFrame({'Features': X.columns, 'Importance': feature_importance})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Identify features with importance below the threshold\n",
        "low_importance_features = feature_importance_df[feature_importance_df['Importance'] < threshold]['Features']\n",
        "\n",
        "X_reduced = X.drop(columns=low_importance_features)\n",
        "X_reduced"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.550353Z",
          "iopub.status.idle": "2024-02-11T05:37:36.550764Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.550571Z",
          "shell.execute_reply": "2024-02-11T05:37:36.550588Z"
        },
        "trusted": true,
        "id": "fxZ2wbT-CybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the redudandant columns\n",
        "columns_to_remove = ['word_count', 'comma_count', 'flesch_reading_score']\n",
        "\n",
        "# Remove the specified columns\n",
        "X_reduced = X_reduced.drop(columns=columns_to_remove, errors='ignore')\n",
        "X_reduced"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.552332Z",
          "iopub.status.idle": "2024-02-11T05:37:36.552748Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.552556Z",
          "shell.execute_reply": "2024-02-11T05:37:36.552574Z"
        },
        "trusted": true,
        "id": "sajL63g7CybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_reduced = scaler.fit_transform(X_reduced)\n",
        "X_reduced"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.554456Z",
          "iopub.status.idle": "2024-02-11T05:37:36.555626Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.555273Z",
          "shell.execute_reply": "2024-02-11T05:37:36.555301Z"
        },
        "trusted": true,
        "id": "ebefJ_v1CybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_reduced_train, X_reduced_valid = train_test_split(X_reduced, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.5585Z",
          "iopub.status.idle": "2024-02-11T05:37:36.559005Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.558771Z",
          "shell.execute_reply": "2024-02-11T05:37:36.558789Z"
        },
        "trusted": true,
        "id": "a7ql0kWrCybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words ❌\n",
        "\n",
        "- Bag of words was used for its simplicity and ease of use\n",
        "- Effective for short to medium texts with a mean word count of 121\n",
        "- Serves as a good baseline\n",
        "- Experimented with:\n",
        "    - Adding meta text features\n",
        "    - Bigrams"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008629,
          "end_time": "2024-02-04T08:13:08.445067",
          "exception": false,
          "start_time": "2024-02-04T08:13:08.436438",
          "status": "completed"
        },
        "tags": [],
        "id": "9kNkH9FiCybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating bag of words representation\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "X_train_bow, X_valid_bow, y_train_bow, y_valid_bow = train_test_split(\n",
        "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
        "\n",
        "clf_bow = LogisticRegression(class_weight='balanced', max_iter=1500)\n",
        "clf_bow.fit(X_train_bow, y_train_bow)\n",
        "y_pred_bow = clf_bow.predict(X_valid_bow)\n",
        "print(\"BoW Classification Report:\\n\", classification_report(y_valid_bow, y_pred_bow))"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.754329,
          "end_time": "2024-02-04T08:13:10.208166",
          "exception": false,
          "start_time": "2024-02-04T08:13:08.453837",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.560469Z",
          "iopub.status.idle": "2024-02-11T05:37:36.561649Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.561393Z",
          "shell.execute_reply": "2024-02-11T05:37:36.561422Z"
        },
        "trusted": true,
        "id": "O1FZS5niCybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_reduced_train, X_reduced_valid\n",
        "\n",
        "X_train_combined = hstack([X_train_bow, X_reduced_train])\n",
        "X_valid_combined = hstack([X_valid_bow, X_reduced_valid])\n",
        "\n",
        "clf_combined = LogisticRegression(class_weight='balanced', max_iter=1500)\n",
        "clf_combined.fit(X_train_combined, y_train)\n",
        "\n",
        "y_pred_combined = clf_combined.predict(X_valid_combined)\n",
        "print(classification_report(y_valid_bow, y_pred_combined))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.56255Z",
          "iopub.status.idle": "2024-02-11T05:37:36.562997Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.562784Z",
          "shell.execute_reply": "2024-02-11T05:37:36.562802Z"
        },
        "trusted": true,
        "id": "v04oHqs_CybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> Relatively stable performance with a very slight increase in precision (for class 2), recall (for class 0), and f1-score (for all classes). <br>\n",
        "--> Indicates that combing bag of words with meta features shows no performance boost. <br>\n",
        "Lets see if ngrams is effective in observing performance gains.\n",
        "</div>"
      ],
      "metadata": {
        "id": "4xKUlfuTCybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating bag of words representation with unigrams + bigrams\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "bow_matrix = count_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "X_train_bow, X_valid_bow, y_train_bow, y_valid_bow = train_test_split(\n",
        "    bow_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
        "\n",
        "clf_bow = LogisticRegression(class_weight='balanced', max_iter=1500)\n",
        "clf_bow.fit(X_train_bow, y_train_bow)\n",
        "y_pred_bow = clf_bow.predict(X_valid_bow)\n",
        "print(\"BoW Classification Report:\\n\", classification_report(y_valid_bow, y_pred_bow))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.566568Z",
          "iopub.status.idle": "2024-02-11T05:37:36.56697Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.566777Z",
          "shell.execute_reply": "2024-02-11T05:37:36.566793Z"
        },
        "trusted": true,
        "id": "N_FPXg43CybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "    --> For Class 0, the inclusion of bigrams led to an improvement in both precision (from 0.96 to 0.95) and recall (from 0.87 to 0.93) for Class 0 which suggests that the bigrams were slightly effective in capturing the contextual information for this class <br><br>\n",
        "    --> For Class 1, there was a slight increase in precision (from 0.08 to 0.12) but a decrease in recall (from 0.19 to 0.09). This indicates that while the model became slightly more precise in its predictions for Class 1 with bigrams, it also became less sensitive to detecting true instances of this class. This could be due to the very large class imbalance and the fact that the class weights are not effective enough at handling it.<br><br>\n",
        "    --> For Class 2, the addition of bigrams resulted in an increase in both precision (from 0.56 to 0.62) and recall (from 0.75 to 0.75) for Class 2. This improvement suggests that bigrams provided valuable context that enhanced the model's ability to accurately classify instances of Class 2.\n",
        "<br><br>\n",
        "We will not try trigrams since the similar performance is to be expected and also because of the long runtime.\n",
        "</div>"
      ],
      "metadata": {
        "id": "wNwFf5FBCybJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF ❌\n",
        "- TF-IDF improves upon the basic Bag of Words model by not just counting words, but by assigning weights to words based on their importance. Common words across all documents receive lower weights, while unique words in specific documents are emphasized (reduction in noise)\n",
        "- Can better capture the relevance of terms within specific reviews. This is crucial in understanding the sentiment and specifics of airline service, where certain terms might be particularly indicative of positive or negative experiences.\n",
        "- Experimented with:\n",
        "    - Adding meta text features\n",
        "    - Bigrams"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008839,
          "end_time": "2024-02-04T08:13:10.226805",
          "exception": false,
          "start_time": "2024-02-04T08:13:10.217966",
          "status": "completed"
        },
        "tags": [],
        "id": "BNc8gGV1CybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a TF-IDF text representation\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "X_train_tfidf, X_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(\n",
        "    tfidf_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
        "\n",
        "clf_tfidf = LogisticRegression(class_weight='balanced', max_iter=2000)\n",
        "clf_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
        "y_pred_tfidf = clf_tfidf.predict(X_valid_tfidf)\n",
        "print(\"TF-IDF Classification Report:\\n\", classification_report(y_valid_tfidf, y_pred_tfidf))"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.824246,
          "end_time": "2024-02-04T08:13:12.059814",
          "exception": false,
          "start_time": "2024-02-04T08:13:10.235568",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.567783Z",
          "iopub.status.idle": "2024-02-11T05:37:36.568223Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.567994Z",
          "shell.execute_reply": "2024-02-11T05:37:36.568012Z"
        },
        "trusted": true,
        "id": "NUjIbKAECybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> Performance wise is already much higher than the bag of words embeddings, especially for the passives/neutral/class 1 in terms of recall. There is a slight dip in precision (by 0.03 to 0.04) for class 1 and 2\n",
        "</div>"
      ],
      "metadata": {
        "id": "EFGJAWiBCybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_reduced_train, X_reduced_valid\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf, X_reduced_train])\n",
        "X_valid_combined = hstack([X_valid_tfidf, X_reduced_valid])\n",
        "\n",
        "clf_combined = LogisticRegression(class_weight='balanced', max_iter=2000)\n",
        "clf_combined.fit(X_train_combined, y_train_tfidf)\n",
        "\n",
        "y_pred_combined = clf_combined.predict(X_valid_combined)\n",
        "print(classification_report(y_valid_tfidf, y_pred_combined))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.569277Z",
          "iopub.status.idle": "2024-02-11T05:37:36.569687Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.569491Z",
          "shell.execute_reply": "2024-02-11T05:37:36.569508Z"
        },
        "trusted": true,
        "id": "qQicHYdiCybJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> No performance gain can be seen in any class when adding the text meta features\n",
        "</div>"
      ],
      "metadata": {
        "id": "jqa37pP8CybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating tfidf representation with unigrams + bigrams\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "X_train_tfidf, X_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(\n",
        "    tfidf_matrix, df[\"NPS_category\"], test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "clf_tfidf = LogisticRegression(class_weight='balanced', max_iter=2000)\n",
        "clf_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
        "y_pred_tfidf = clf_tfidf.predict(X_valid_tfidf)\n",
        "print(\"TF-IDF Classification Report:\\n\", classification_report(y_valid_tfidf, y_pred_tfidf))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.570795Z",
          "iopub.status.idle": "2024-02-11T05:37:36.57121Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.570992Z",
          "shell.execute_reply": "2024-02-11T05:37:36.571009Z"
        },
        "trusted": true,
        "id": "tQXaYxyKCybK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> After adding in bigrams the model is able to predict class 2 better than before, however there is a significant drop in recall for class 1 which is of concern. <br>\n",
        "--> Since all the classes are equally important, tfidf on its own is not very suitable. A new technique has been research and found called delta tfidf which will be explore below.\n",
        "</div>"
      ],
      "metadata": {
        "id": "gv83VTyOCybK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Tfidf ✅\n",
        "\n",
        "- Enhanced Feature Discrimination: Delta TF-IDF amplifies the features (words) that are more discriminative for each class by adjusting their TF-IDF values based on their deviation from a mean IDF calculated across all classes leading to better separation of classes.\n",
        "- However, one thing to note is that delta TF-IDF has negative values which means it cannot be used with naive bayes models. An alternative is to use Gaussian Bayes, however it doesnt support class weights, which makes it unsuitable to use."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010003,
          "end_time": "2024-02-04T08:13:12.080169",
          "exception": false,
          "start_time": "2024-02-04T08:13:12.070166",
          "status": "completed"
        },
        "tags": [],
        "id": "cKVhR16fCybN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(df['clean_text'], df['NPS_category'], test_size=0.25, random_state=42)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "class_idfs = []\n",
        "unique_classes = np.unique(y_train)\n",
        "for class_idx in unique_classes:\n",
        "    class_mask = (y_train == class_idx)\n",
        "    class_docs = X_train[class_mask]\n",
        "    class_vectorizer = TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)\n",
        "    class_tfidf = class_vectorizer.fit_transform(class_docs)\n",
        "    class_idf = class_vectorizer.idf_\n",
        "    class_idfs.append(class_idf)\n",
        "\n",
        "mean_idf = np.mean(class_idfs, axis=0)\n",
        "\n",
        "delta_idfs = [class_idf - mean_idf for class_idf in class_idfs]\n",
        "\n",
        "class_to_index = {label: index for index, label in enumerate(unique_classes)}\n",
        "\n",
        "X_train_delta_tfidf = []\n",
        "for i, doc in enumerate(X_train_tfidf):\n",
        "    class_idx = class_to_index[y_train.iloc[i]]\n",
        "    delta_idf = delta_idfs[class_idx]\n",
        "    X_train_delta_tfidf.append(doc.multiply(delta_idf))\n",
        "\n",
        "X_train_delta_tfidf = vstack(X_train_delta_tfidf)\n",
        "\n",
        "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
        "X_valid_delta_tfidf = []\n",
        "for i, doc in enumerate(X_valid_tfidf):\n",
        "    class_idx = class_to_index[y_valid.iloc[i]]\n",
        "    delta_idf = delta_idfs[class_idx]\n",
        "    X_valid_delta_tfidf.append(doc.multiply(delta_idf))\n",
        "\n",
        "X_valid_delta_tfidf = vstack(X_valid_delta_tfidf)\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced')\n",
        "clf.fit(X_train_delta_tfidf, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid_delta_tfidf)\n",
        "print(classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "papermill": {
          "duration": 22.571828,
          "end_time": "2024-02-04T08:13:34.6616",
          "exception": false,
          "start_time": "2024-02-04T08:13:12.089772",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.572954Z",
          "iopub.status.idle": "2024-02-11T05:37:36.573491Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.573144Z",
          "shell.execute_reply": "2024-02-11T05:37:36.573159Z"
        },
        "trusted": true,
        "id": "sGlOC0PGCybN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> A much signficant performance upgrade can be seen across all the classes. Since adding meta features and bigrams didnt make much of a difference for tfidf, it is very likely that it will not make much difference for delta tfidf as well.\n",
        "</div>"
      ],
      "metadata": {
        "id": "i9CIoim5CybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Balancing Techniques\n",
        "Dataset is highly imbalanced which means that the predictions will be highly biased towards the majority class. To fix this it is important to balance the dataset through:\n",
        "\n",
        "- using class weights\n",
        "- using SMOTE\n",
        "- using adasyn\n",
        "\n",
        "*note: random oversampling was not used since it works by duplicating the samples in the minority class, even if it has better performance, it is most likely overfitting.*"
      ],
      "metadata": {
        "tags": [],
        "id": "WL5kDZ-uCybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Tfidf + class weights (Balancing Technique) ✅"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017221,
          "end_time": "2024-02-04T08:13:34.705378",
          "exception": false,
          "start_time": "2024-02-04T08:13:34.688157",
          "status": "completed"
        },
        "tags": [],
        "id": "skcesBG0CybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "clf.fit(X_train_delta_tfidf, y_train)\n",
        "y_pred = clf.predict(X_valid_delta_tfidf)\n",
        "\n",
        "print(classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "papermill": {
          "duration": 6.743267,
          "end_time": "2024-02-04T08:13:41.465593",
          "exception": false,
          "start_time": "2024-02-04T08:13:34.722326",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.575436Z",
          "iopub.status.idle": "2024-02-11T05:37:36.575962Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.575703Z",
          "shell.execute_reply": "2024-02-11T05:37:36.575727Z"
        },
        "trusted": true,
        "id": "DxmRRWXECybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Tfidf + SMOTE (Balancing Technique) ❌\n",
        "- should not be applied on the test data to preserve the original distribution."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010394,
          "end_time": "2024-02-04T08:13:54.185053",
          "exception": false,
          "start_time": "2024-02-04T08:13:54.174659",
          "status": "completed"
        },
        "tags": [],
        "id": "5MuQmnASCybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_delta_tfidf, y_train)\n",
        "\n",
        "clf_smote = LogisticRegression()\n",
        "clf_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred = clf_smote.predict(X_valid_delta_tfidf)\n",
        "\n",
        "print(\"Classification Report after SMOTE:\\n\", classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "papermill": {
          "duration": 15.574854,
          "end_time": "2024-02-04T08:14:09.769864",
          "exception": false,
          "start_time": "2024-02-04T08:13:54.19501",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.577819Z",
          "iopub.status.idle": "2024-02-11T05:37:36.578269Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.578043Z",
          "shell.execute_reply": "2024-02-11T05:37:36.578061Z"
        },
        "trusted": true,
        "id": "xrNPTZTICybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> A low precision (high false positives) is seen for class 1, this shows that SMOTE is not the best technique to apply.\n",
        "</div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "1SHm8JiVCybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delta Tfidf + Adasyn (Balancing Technique) ❌\n",
        "- should not be applied on the test data to preserve the original distribution."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016124,
          "end_time": "2024-02-04T08:14:09.80725",
          "exception": false,
          "start_time": "2024-02-04T08:14:09.791126",
          "status": "completed"
        },
        "tags": [],
        "id": "rl0h67TsCybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_delta_tfidf, y_train)\n",
        "\n",
        "clf_adasyn = LogisticRegression()\n",
        "clf_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
        "y_pred = clf_adasyn.predict(X_valid_delta_tfidf)\n",
        "\n",
        "print(\"Classification Report after ADASYN:\\n\", classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "papermill": {
          "duration": 57.043174,
          "end_time": "2024-02-04T08:15:06.861416",
          "exception": false,
          "start_time": "2024-02-04T08:14:09.818242",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.579627Z",
          "iopub.status.idle": "2024-02-11T05:37:36.580074Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.57987Z",
          "shell.execute_reply": "2024-02-11T05:37:36.579888Z"
        },
        "trusted": true,
        "id": "1KeKt_NZCybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> A low precision (high false positives) is seen for class 1, this shows that ADASYN is not the best technique to apply. Class weighting will be the selected technique. This means that for modelling it is important to utilise models that support adding weights for the classes.\n",
        "</div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "YuL2F6BgCybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling\n",
        "\n",
        "Some considerations when building models:\n",
        "\n",
        "- The SVM classifier can be built using LinearSVC and Stochastic gradient descent with hinge loss. LinearSVC uses the full data to solve the optimization problem which makes it suitable on small datasets however, on larger and more sparse data, SGD (with hinge loss) is more suitable as it handles data in batches assuming data points are independent and identically distributed.\n",
        "\n",
        "- Boosting algorithms comparison\n",
        "    - for Adaboost, 2 different stump learners was used which was decision tree (most commonly used) and logistic regression (better baseline performance than decision tree).\n",
        "    - LightGBM, Catboost and XGBoost is used due to its fast performance and large number of tunable parameters.\n",
        "    \n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0xWny_DiMZxN3Ajpg6G5tg.png\" style=\"width: 800px; height: 400px;\">\n",
        "</div>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009547,
          "end_time": "2024-02-04T08:15:06.882698",
          "exception": false,
          "start_time": "2024-02-04T08:15:06.873151",
          "status": "completed"
        },
        "tags": [],
        "id": "XfpWILMRCybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight='balanced', max_iter=2000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
        "    \"SVM (SGD)\": SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3),\n",
        "    \"Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
        "    \"XGBoost\": XGBClassifier(enable_categorical=True, eval_metric='mlogloss'),\n",
        "    \"AdaBoost+DT\": AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100),\n",
        "    \"AdaBoost+LR\": AdaBoostClassifier(base_estimator=LogisticRegression(multi_class='multinomial',class_weight='balanced',max_iter=1000),n_estimators=100),\n",
        "#     \"Catboost\": CatBoostClassifier(iterations=500,learning_rate=0.1,depth=6,l2_leaf_reg=3,cat_features=[], auto_class_weights='Balanced',verbose=200, task_type='GPU',devices='0:1'),\n",
        "#     \"LightGBM\": LGBMClassifier(boosting_type='gbdt', objective='multiclass', class_weight='balanced', learning_rate=0.09, max_depth=-1, random_state=42, n_estimators=100, device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
        "}\n",
        "\n",
        "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
        "    if name == \"XGBoost\":\n",
        "        sample_weights_data = compute_sample_weight(\"balanced\", y_train)\n",
        "        model.fit(X_train, y_train, sample_weight=sample_weights_data)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(f\"CLASSIFICATION REPORT FOR {name.upper()}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return {\"Model\": name, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.4164,
          "end_time": "2024-02-04T08:15:07.309116",
          "exception": false,
          "start_time": "2024-02-04T08:15:06.892716",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.581048Z",
          "iopub.status.idle": "2024-02-11T05:37:36.581617Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.581275Z",
          "shell.execute_reply": "2024-02-11T05:37:36.581372Z"
        },
        "trusted": true,
        "id": "f24luS2kCybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame()\n",
        "\n",
        "for name, model in tqdm(models.items(), desc=\"Evaluating Models\"):\n",
        "    results_row = evaluate_model(name, model, X_train_delta_tfidf, y_train, X_valid_delta_tfidf, y_valid)\n",
        "    results = results._append(results_row, ignore_index=True)\n",
        "\n",
        "results"
      ],
      "metadata": {
        "papermill": {
          "duration": 216.137957,
          "end_time": "2024-02-04T08:18:43.457097",
          "exception": false,
          "start_time": "2024-02-04T08:15:07.31914",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-11T05:37:36.583274Z",
          "iopub.status.idle": "2024-02-11T05:37:36.583683Z",
          "shell.execute_reply.started": "2024-02-11T05:37:36.583488Z",
          "shell.execute_reply": "2024-02-11T05:37:36.583506Z"
        },
        "trusted": true,
        "id": "fghmMBDzCybO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "--> SGD (SVM) and Random Forest exhibit the highest F1-Score, making them top performers for precise classification. <br>\n",
        "--> Logistic Regression is close behind, with strong metrics, suggesting it's a reliable choice.<br>\n",
        "--> Decision Tree shows lower performance, which might indicate it's less suited for this task.<br>\n",
        "--> XGBoost underperforms in this comparison, hinting at the need for more optimization.<br>\n",
        "</div>"
      ],
      "metadata": {
        "id": "BRkw6a1CCybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality reduction techniques"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "papermill": {
          "duration": 0.016571,
          "end_time": "2024-02-04T08:18:43.49098",
          "exception": false,
          "start_time": "2024-02-04T08:18:43.474409",
          "status": "completed"
        },
        "tags": [],
        "id": "NVJJ91AvCybO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVD"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01732,
          "end_time": "2024-02-04T08:18:43.525783",
          "exception": false,
          "start_time": "2024-02-04T08:18:43.508463",
          "status": "completed"
        },
        "tags": [],
        "id": "JhYd6c-uCybP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Step 1: Apply Truncated SVD for feature reduction on CPU\n",
        "n_components = 50  # Adjust the number of components\n",
        "n_iter = 10  # Adjust the number of iterations\n",
        "random_state = 42  # Set a fixed random state for reproducibility\n",
        "algorithm = 'randomized'  # Experiment with different algorithms\n",
        "\n",
        "svd = TruncatedSVD(\n",
        "    n_components=n_components,\n",
        "    n_iter=n_iter,\n",
        "    random_state=random_state,\n",
        "    algorithm=algorithm\n",
        ")\n",
        "\n",
        "# Fit and transform on training data\n",
        "X_train_svd = svd.fit_transform(X_train_delta_tfidf.toarray())\n",
        "\n",
        "# Transform on test data\n",
        "X_valid_svd = svd.transform(X_valid_delta_tfidf.toarray())\n",
        "print(\"SVD finished\")\n",
        "\n",
        "# Step 2: Logistic Regression with class weights for evaluation\n",
        "logistic_regression = LogisticRegression(class_weight='balanced')\n",
        "print(\"logreg init\")\n",
        "\n",
        "# Fit the model\n",
        "logistic_regression.fit(X_train_svd, y_train)\n",
        "print(\"logreg fit\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = logistic_regression.predict(X_valid_svd)\n",
        "print(\"logreg predict\")\n",
        "\n",
        "# Step 3: Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
        "print(classification_report(y_valid, y_pred))\n",
        "\n",
        "# Step 4: Visualize SVD results\n",
        "plt.scatter(X_train_svd[:, 0], X_train_svd[:, 1], c=y_train, cmap=plt.cm.Paired)\n",
        "plt.colorbar()\n",
        "plt.title(\"SVD Visualization of Training Data\")\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(X_valid_svd[:, 0], X_valid_svd[:, 1], c=y_test, cmap=plt.cm.Paired)\n",
        "plt.colorbar()\n",
        "plt.title(\"SVD Visualization of Test Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 96.41485,
          "end_time": "2024-02-04T08:20:19.95799",
          "exception": false,
          "start_time": "2024-02-04T08:18:43.54314",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T08:40:10.549532Z",
          "iopub.status.idle": "2024-02-10T08:40:10.550175Z",
          "shell.execute_reply.started": "2024-02-10T08:40:10.549871Z",
          "shell.execute_reply": "2024-02-10T08:40:10.549897Z"
        },
        "trusted": true,
        "id": "T5cmfFfrCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01745,
          "end_time": "2024-02-04T08:20:20.068761",
          "exception": false,
          "start_time": "2024-02-04T08:20:20.051311",
          "status": "completed"
        },
        "tags": [],
        "id": "6QiZauduCybP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/kaggle/input/dataprocessing/testing_data.csv\", index_col=0)\n",
        "# test_df = pd.read_csv(\"./DataScraping_and_processing/testing_data.csv\", index_col=0)\n",
        "test_df.head()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016624,
          "end_time": "2024-02-04T08:20:20.102068",
          "exception": false,
          "start_time": "2024-02-04T08:20:20.085444",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T09:50:26.831331Z",
          "iopub.execute_input": "2024-02-10T09:50:26.83198Z",
          "iopub.status.idle": "2024-02-10T09:50:27.049182Z",
          "shell.execute_reply.started": "2024-02-10T09:50:26.831947Z",
          "shell.execute_reply": "2024-02-10T09:50:27.04782Z"
        },
        "trusted": true,
        "id": "EEzB1fSkCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T09:50:27.051303Z",
          "iopub.execute_input": "2024-02-10T09:50:27.051664Z",
          "iopub.status.idle": "2024-02-10T09:50:27.05965Z",
          "shell.execute_reply.started": "2024-02-10T09:50:27.051633Z",
          "shell.execute_reply": "2024-02-10T09:50:27.058285Z"
        },
        "trusted": true,
        "id": "du3agzluCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"review_text\"].dropna()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T09:50:27.198163Z",
          "iopub.execute_input": "2024-02-10T09:50:27.19883Z",
          "iopub.status.idle": "2024-02-10T09:50:27.211687Z",
          "shell.execute_reply.started": "2024-02-10T09:50:27.198792Z",
          "shell.execute_reply": "2024-02-10T09:50:27.210335Z"
        },
        "trusted": true,
        "id": "sSr4YMUNCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dict = {}\n",
        "word_freq = Counter()\n",
        "\n",
        "with open('/kaggle/input/dataprocessing/slang.txt', 'r') as file:\n",
        "    slang_dict = {key.lower(): value.lower() for line in file if '=' in line for key, value in [line.strip().split('=', 1)]}\n",
        "\n",
        "# with open('DataScraping_and_processing/slang.txt', 'r') as file:\n",
        "#     slang_dict = {key.lower(): value.lower() for line in file if '=' in line for key, value in [line.strip().split('=', 1)]}\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review_text'])\n",
        "\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "tfidf_sum = np.sum(tfidf_matrix, axis=0)\n",
        "tfidf_scores = np.squeeze(np.asarray(tfidf_sum))\n",
        "\n",
        "sorted_indices = np.argsort(tfidf_scores)\n",
        "\n",
        "freq_word_threshold = np.quantile(tfidf_scores, 0.99)\n",
        "rare_word_threshold = np.quantile(tfidf_scores, 0.01)\n",
        "\n",
        "FREQWORDS = set(feature_names[tfidf_scores > freq_word_threshold])\n",
        "RAREWORDS = set(feature_names[tfidf_scores < rare_word_threshold])\n",
        "\n",
        "print(\"Frequent Words:\", list(FREQWORDS)[:5])\n",
        "print(\"\\nRare Words:\", list(RAREWORDS)[:5])\n",
        "\n",
        "def detect_language(text):\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang\n",
        "\n",
        "tqdm.pandas(desc=\"Detecting languages\")\n",
        "test_df['language_info'] = test_df['review_text'].progress_apply(detect_language)\n",
        "test_df.loc[test_df[\"language_info\"] != \"en\"]\n",
        "\n",
        "def remove_rows(dataframe):\n",
        "    print(\"rows removed\")\n",
        "    return (dataframe.dropna()\n",
        "            .drop_duplicates()\n",
        "            .query(\"language_info == 'en'\")\n",
        "            .loc[dataframe['review_text'].str.split().str.len() > 2])\n",
        "\n",
        "data_cleaning_pipeline = Pipeline([\n",
        "    ('remove_rows', FunctionTransformer(remove_rows)),\n",
        "])\n",
        "\n",
        "# text cleaning functions\n",
        "\n",
        "def remove_excess_space(text):\n",
        "    text = \" \".join(text.split())\n",
        "    text = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def remove_url(text):\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    return re.sub(regex, '', text)\n",
        "\n",
        "def html_tag_remover(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def normalize_exaggerated_text(text):\n",
        "    normalized_text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "    return normalized_text\n",
        "\n",
        "def replace_slang(text):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in slang_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: slang_dict[x.group().lower()], text)\n",
        "\n",
        "def convert_emoticons_and_emojis(text, emoticons_emo=EMOTICONS_EMO, emoji_unicode=EMOJI_UNICODE):\n",
        "    emoticon_patterns = {re.escape(k): \"_\".join(v.replace(\",\", \"\").split()) for k, v in emoticons_emo.items()}\n",
        "    emoji_patterns = {re.escape(v): k.strip(\":\") for k, v in emoji_unicode.items()}\n",
        "    combined_patterns = {**emoticon_patterns, **emoji_patterns}\n",
        "\n",
        "    pattern = re.compile('|'.join(combined_patterns.keys()))\n",
        "\n",
        "    def replace(match):\n",
        "        return combined_patterns[re.escape(match.group(0))]\n",
        "\n",
        "    return pattern.sub(replace, text)\n",
        "\n",
        "def remove_username(text):\n",
        "    return re.sub('@[^\\s]+','',text)\n",
        "\n",
        "symsp = SymSpell()\n",
        "symsp.load_dictionary('/kaggle/input/dataprocessing/my_frequency_dictionary.txt', term_index=0, count_index=1, separator=' ')\n",
        "\n",
        "def correct_spelling(text):\n",
        "    corrected = symsp.lookup_compound(text, max_edit_distance=2)\n",
        "    return corrected[0].term if corrected else text\n",
        "\n",
        "def remove_money(text):\n",
        "    pattern = r'(?:\\$|\\£|\\€|\\₹)?\\d+(?:\\.\\d{1,2})?'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "def fix_negation(sentence):\n",
        "    for i in range(1, len(sentence)):\n",
        "        if sentence[i-1] in ['not', \"n't\"]:\n",
        "            synsets = wordnet.synsets(sentence[i])\n",
        "            if synsets:\n",
        "                w1 = synsets[0]\n",
        "                antonyms = [l.antonyms()[0].name() for l in w1.lemmas() if l.antonyms()]\n",
        "                if antonyms:\n",
        "                    max_dissimilarity = 0\n",
        "                    for ant in antonyms:\n",
        "                        ant_synsets = wordnet.synsets(ant)\n",
        "                        if ant_synsets:\n",
        "                            similarity = w1.wup_similarity(ant_synsets[0])\n",
        "                            # Check if similarity is None and handle it\n",
        "                            if similarity is None:\n",
        "                                continue  # Skip this comparison\n",
        "                            dissimilarity = 1 - similarity\n",
        "                            if dissimilarity > max_dissimilarity:\n",
        "                                max_dissimilarity = dissimilarity\n",
        "                                sentence[i] = ant\n",
        "                    sentence[i-1] = ''\n",
        "\n",
        "    sentence = [word for word in sentence if word]\n",
        "    return ' '.join(sentence)  # Return a string instead of a list\n",
        "\n",
        "\n",
        "STOPWORDS = stopwords.words(\"english\")\n",
        "STOPWORDS += ['u', 'back', 'one']\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "def remove_freqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "\n",
        "def remove_rarewords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "text_preprocessing_pipeline = Pipeline(steps=[\n",
        "    ('lowercase', FunctionTransformer(lambda x: x.progress_apply(str.lower))),\n",
        "    ('remove excess whitespace', FunctionTransformer(lambda x: x.progress_apply(remove_excess_space))),\n",
        "    ('remove_url', FunctionTransformer(lambda x: x.progress_apply(remove_url))),\n",
        "    ('html_tag_remover', FunctionTransformer(lambda x: x.progress_apply(html_tag_remover))),\n",
        "    ('money_remover', FunctionTransformer(lambda x: x.progress_apply(remove_money))),\n",
        "    ('convert_emoticons_and_emojii', FunctionTransformer(lambda x: x.progress_apply(convert_emoticons_and_emojis))),\n",
        "    ('remove_username', FunctionTransformer(lambda x: x.progress_apply(remove_username))),\n",
        "    ('replace_slang', FunctionTransformer(lambda x: x.progress_apply(replace_slang))),\n",
        "    ('correct_spelling', FunctionTransformer(lambda x: x.progress_apply(correct_spelling))),\n",
        "    ('expand_contractions', FunctionTransformer(lambda x: x.progress_apply(lambda text: contractions.fix(text)))),\n",
        "    ('fix_negation', FunctionTransformer(lambda x: x.progress_apply(lambda text: \"\".join(fix_negation(word_tokenize(text)))))),\n",
        "    ('remove_punctuation', FunctionTransformer(lambda x: x.progress_apply(remove_punctuation))),\n",
        "    ('normalize_exaggerated_text', FunctionTransformer(lambda x: x.progress_apply(normalize_exaggerated_text))),\n",
        "    ('remove_stopwords', FunctionTransformer(lambda x: x.progress_apply(remove_stopwords))),\n",
        "    ('remove_freqwords', FunctionTransformer(lambda x: x.progress_apply(remove_freqwords))),\n",
        "    ('remove_rarewords', FunctionTransformer(lambda x: x.progress_apply(remove_rarewords))),\n",
        "    ('lemmatize_words', FunctionTransformer(lambda x: x.progress_apply(lemmatize_words)))\n",
        "])"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T09:50:27.410226Z",
          "iopub.execute_input": "2024-02-10T09:50:27.410631Z",
          "iopub.status.idle": "2024-02-10T09:51:56.738969Z",
          "shell.execute_reply.started": "2024-02-10T09:50:27.4106Z",
          "shell.execute_reply": "2024-02-10T09:51:56.737918Z"
        },
        "trusted": true,
        "id": "FFvZbjq6CybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = data_cleaning_pipeline.fit_transform(test_df)\n",
        "test_df['clean_text'] = text_preprocessing_pipeline.fit_transform(test_df['review_text'])\n",
        "test_df.head()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T09:51:56.740842Z",
          "iopub.execute_input": "2024-02-10T09:51:56.742088Z",
          "iopub.status.idle": "2024-02-10T10:04:29.949767Z",
          "shell.execute_reply.started": "2024-02-10T09:51:56.742052Z",
          "shell.execute_reply": "2024-02-10T10:04:29.948276Z"
        },
        "trusted": true,
        "id": "SiQLE78nCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_nps(rating_text):\n",
        "    rating_text = rating_text.strip()\n",
        "    if \"Rated\" in rating_text:\n",
        "        rating = int(rating_text.split(' ')[1])\n",
        "        return \"Detractor\" if rating <= 2 else (\"Neutral\" if rating == 3 else \"Promoter\")\n",
        "    else:\n",
        "        rating = int(float(rating_text))\n",
        "        return \"Detractor\" if rating <= 6 else (\"Neutral\" if rating <= 8 else \"Promoter\")\n",
        "\n",
        "\n",
        "\n",
        "test_df[\"NPS_category\"] = test_df[\"overall_rating\"].apply(classify_nps)\n",
        "test_df[\"NPS_category\"] = test_df[\"NPS_category\"].map({\"Detractor\": 0, \"Neutral\": 1, \"Promoter\": 2})\n",
        "test_df.head()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:04:29.951856Z",
          "iopub.execute_input": "2024-02-10T10:04:29.95236Z",
          "iopub.status.idle": "2024-02-10T10:04:29.997665Z",
          "shell.execute_reply.started": "2024-02-10T10:04:29.952313Z",
          "shell.execute_reply": "2024-02-10T10:04:29.996266Z"
        },
        "trusted": true,
        "id": "CFyG-4lwCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.reset_index(drop=True)\n",
        "test_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:04:30.000724Z",
          "iopub.execute_input": "2024-02-10T10:04:30.001271Z",
          "iopub.status.idle": "2024-02-10T10:04:30.037028Z",
          "shell.execute_reply.started": "2024-02-10T10:04:30.001194Z",
          "shell.execute_reply": "2024-02-10T10:04:30.035746Z"
        },
        "trusted": true,
        "id": "YGv9oVHYCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.dropna(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:07:10.700298Z",
          "iopub.execute_input": "2024-02-10T10:07:10.700745Z",
          "iopub.status.idle": "2024-02-10T10:07:10.7219Z",
          "shell.execute_reply.started": "2024-02-10T10:07:10.700712Z",
          "shell.execute_reply": "2024-02-10T10:07:10.720699Z"
        },
        "trusted": true,
        "id": "tLB0SR1YCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/working/cleaned_dataset_v2.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:09:44.361294Z",
          "iopub.execute_input": "2024-02-10T10:09:44.361751Z",
          "iopub.status.idle": "2024-02-10T10:09:45.796709Z",
          "shell.execute_reply.started": "2024-02-10T10:09:44.361716Z",
          "shell.execute_reply": "2024-02-10T10:09:45.795364Z"
        },
        "trusted": true,
        "id": "3pJW0jWFCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:12:44.301607Z",
          "iopub.execute_input": "2024-02-10T10:12:44.302087Z",
          "iopub.status.idle": "2024-02-10T10:12:44.365413Z",
          "shell.execute_reply.started": "2024-02-10T10:12:44.302052Z",
          "shell.execute_reply": "2024-02-10T10:12:44.364013Z"
        },
        "trusted": true,
        "id": "I5N9RMXKCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(df['clean_text'], df['NPS_category'], test_size=0.25, random_state=42)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase=True)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "class_idfs = []\n",
        "unique_classes = np.unique(y_train)\n",
        "for class_idx in unique_classes:\n",
        "    class_mask = (y_train == class_idx)\n",
        "    class_docs = X_train[class_mask]\n",
        "    class_vectorizer = TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_, lowercase=True)\n",
        "    class_tfidf = class_vectorizer.fit_transform(class_docs)\n",
        "    class_idf = class_vectorizer.idf_\n",
        "    class_idfs.append(class_idf)\n",
        "\n",
        "mean_idf = np.mean(class_idfs, axis=0)\n",
        "\n",
        "delta_idfs = [class_idf - mean_idf for class_idf in class_idfs]\n",
        "\n",
        "class_to_index = {label: index for index, label in enumerate(unique_classes)}\n",
        "X_train_delta_tfidf = []\n",
        "for i, doc in enumerate(X_train_tfidf):\n",
        "    class_idx = class_to_index[y_train.iloc[i]]\n",
        "    delta_idf = delta_idfs[class_idx]\n",
        "    X_train_delta_tfidf.append(doc.multiply(delta_idf))\n",
        "\n",
        "X_train_delta_tfidf = vstack(X_train_delta_tfidf)\n",
        "\n",
        "clf = SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3)\n",
        "clf.fit(X_train_delta_tfidf, y_train)"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:12:52.725109Z",
          "iopub.execute_input": "2024-02-10T10:12:52.725562Z",
          "iopub.status.idle": "2024-02-10T10:13:09.825384Z",
          "shell.execute_reply.started": "2024-02-10T10:12:52.72553Z",
          "shell.execute_reply": "2024-02-10T10:13:09.824224Z"
        },
        "trusted": true,
        "id": "1cp5ESqPCybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['review_text'])\n",
        "y_test = test_df['NPS_category']\n",
        "\n",
        "\n",
        "X_test_delta_tfidf = []\n",
        "for i, doc in enumerate(X_test_tfidf):\n",
        "    class_idx = class_to_index[y_test.iloc[i]]\n",
        "    delta_idf = delta_idfs[class_idx]\n",
        "    X_test_delta_tfidf.append(doc.multiply(delta_idf))\n",
        "\n",
        "X_test_delta_tfidf = vstack(X_test_delta_tfidf)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test_delta_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:13:12.439816Z",
          "iopub.execute_input": "2024-02-10T10:13:12.440291Z",
          "iopub.status.idle": "2024-02-10T10:13:19.227367Z",
          "shell.execute_reply.started": "2024-02-10T10:13:12.440252Z",
          "shell.execute_reply": "2024-02-10T10:13:19.225948Z"
        },
        "trusted": true,
        "id": "ic3FGH37CybP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning\n",
        "\n",
        "\n",
        "sgd_clf = SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3)\n",
        "\n",
        "param_dist = {\n",
        "    'alpha': loguniform(1e-7, 1e-2),  # Broaden the range a bit to include smaller and larger values\n",
        "    'learning_rate': ['optimal', 'invscaling', 'adaptive'],  # This is fine as is, covering all the options\n",
        "    'eta0': loguniform(1e-6, 1e-1),  # Adjust the range for initial learning rate to be broader\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],  # Including all relevant penalties\n",
        "    'l1_ratio': uniform(0, 1),\n",
        "    'power_t': uniform(0.1, 0.9)  # For 'invscaling' learning rate, exploring a range for power_t could be beneficial\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(sgd_clf, param_distributions=param_dist, n_iter=50,\n",
        "                                   scoring='f1_weighted', n_jobs=-1, cv=5, random_state=42)\n",
        "\n",
        "\n",
        "random_search.fit(X_train_delta_tfidf, y_train)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "y_pred = best_model.predict(X_test_delta_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:13:25.354811Z",
          "iopub.execute_input": "2024-02-10T10:13:25.355733Z",
          "iopub.status.idle": "2024-02-10T10:17:08.470646Z",
          "shell.execute_reply.started": "2024-02-10T10:13:25.355691Z",
          "shell.execute_reply": "2024-02-10T10:17:08.468886Z"
        },
        "trusted": true,
        "id": "CobQvXzzCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", xticklabels=unique_classes, yticklabels=unique_classes)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Labels')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:08.473917Z",
          "iopub.execute_input": "2024-02-10T10:17:08.474362Z",
          "iopub.status.idle": "2024-02-10T10:17:08.834137Z",
          "shell.execute_reply.started": "2024-02-10T10:17:08.474323Z",
          "shell.execute_reply": "2024-02-10T10:17:08.832724Z"
        },
        "trusted": true,
        "id": "eZTx-UA4CybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "index = np.arange(len(unique_classes))\n",
        "bar_width = 0.2\n",
        "\n",
        "p_bar = ax.bar(index, precision, bar_width, label='Precision')\n",
        "r_bar = ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
        "f1_bar = ax.bar(index + 2 * bar_width, f1, bar_width, label='F1-score')\n",
        "\n",
        "ax.set_xlabel('Classes')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Precision, Recall, and F1-Score per Class')\n",
        "ax.set_xticks(index + bar_width)\n",
        "ax.set_xticklabels(unique_classes, rotation=45)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:08.836545Z",
          "iopub.execute_input": "2024-02-10T10:17:08.836957Z",
          "iopub.status.idle": "2024-02-10T10:17:09.208724Z",
          "shell.execute_reply.started": "2024-02-10T10:17:08.836923Z",
          "shell.execute_reply": "2024-02-10T10:17:09.20717Z"
        },
        "trusted": true,
        "id": "oJyNyl4ECybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Binarize the output\n",
        "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
        "n_classes = y_test_binarized.shape[1]\n",
        "\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "y_score = clf.decision_function(X_test_delta_tfidf)\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Assuming your classes are numbered from 0 to n-1\n",
        "unique_classes = np.arange(n_classes)"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:09.212596Z",
          "iopub.execute_input": "2024-02-10T10:17:09.21315Z",
          "iopub.status.idle": "2024-02-10T10:17:09.259829Z",
          "shell.execute_reply.started": "2024-02-10T10:17:09.213096Z",
          "shell.execute_reply": "2024-02-10T10:17:09.258434Z"
        },
        "trusted": true,
        "id": "nKLswaijCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.io as pio\n",
        "\n",
        "# Set the default renderer to 'iframe' for Jupyter Notebooks or 'notebook' if using JupyterLab\n",
        "pio.renderers.default = 'iframe'  # or 'notebook' for JupyterLab\n",
        "\n",
        "# Initialize the plotly figure with a title and a single subplot for the ROC curves\n",
        "fig = make_subplots(rows=1, cols=1, subplot_titles=['Multiclass ROC Curve'])\n",
        "\n",
        "# Define a list of colors for the ROC curves\n",
        "color_palette = [\n",
        "    '#d62728',  # Muted blue\n",
        "    '#ff7f0e',  # Safety orange\n",
        "    '#2ca02c',  # Cooked asparagus green\n",
        "    '#d62728',  # Brick red\n",
        "    '#9467bd',  # Muted purple\n",
        "    '#8c564b',  # Chestnut brown\n",
        "    '#e377c2',  # Raspberry yogurt pink\n",
        "    '#7f7f7f',  # Middle gray\n",
        "    '#bcbd22',  # Curry yellow-green\n",
        "    '#17becf'   # Blue-teal\n",
        "]\n",
        "\n",
        "for i, color in enumerate(color_palette[:len(unique_classes)]):\n",
        "    fig.add_trace(go.Scatter(x=fpr[i], y=tpr[i], mode='lines',\n",
        "                             name=f'Class {unique_classes[i]} (AUC = {roc_auc[i]:.2f})',\n",
        "                             line=dict(color=color, width=2)))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Chance',\n",
        "                         line=dict(color='gray', width=1.5, dash='dot')))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Multiclass ROC Curve',\n",
        "    title_x=0.5,\n",
        "    xaxis_title='False Positive Rate',\n",
        "    yaxis_title='True Positive Rate',\n",
        "    legend_title='Class AUC',\n",
        "    legend=dict(y=0.5, traceorder='reversed', font_size=12),\n",
        "    margin=dict(l=40, r=40, t=85, b=40),\n",
        "    paper_bgcolor='white',\n",
        "    plot_bgcolor='white',\n",
        "    font=dict(size=12, color='black')\n",
        ")\n",
        "\n",
        "fig.update_xaxes(range=[0, 1], showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
        "fig.update_yaxes(range=[0, 1.05], showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:09.262417Z",
          "iopub.execute_input": "2024-02-10T10:17:09.262914Z",
          "iopub.status.idle": "2024-02-10T10:17:09.368406Z",
          "shell.execute_reply.started": "2024-02-10T10:17:09.262867Z",
          "shell.execute_reply": "2024-02-10T10:17:09.366975Z"
        },
        "trusted": true,
        "id": "_MP2_7PjCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.show(\"svg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:09.369976Z",
          "iopub.execute_input": "2024-02-10T10:17:09.370373Z",
          "iopub.status.idle": "2024-02-10T10:17:09.542019Z",
          "shell.execute_reply.started": "2024-02-10T10:17:09.37034Z",
          "shell.execute_reply": "2024-02-10T10:17:09.540787Z"
        },
        "trusted": true,
        "id": "bllFSfTCCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.base import clone\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "X_csr = csr_matrix(X_test_delta_tfidf)\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "n_splits = 5\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Collect F1 scores for each fold and class\n",
        "f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(X_csr, y_test):\n",
        "    X_train_fold, X_test_fold = X_csr[train_idx], X_csr[test_idx]\n",
        "    y_train_fold, y_test_fold = y_test.iloc[train_idx], y_test.iloc[test_idx]\n",
        "\n",
        "    # Fit and predict\n",
        "    model = clone(best_model)\n",
        "    model.fit(X_train_fold, y_train_fold)\n",
        "    y_pred_fold = model.predict(X_test_fold)\n",
        "\n",
        "    # Compute F1-scores for each class and append to list\n",
        "    f1_scores.append(f1_score(y_test_fold, y_pred_fold, average=None))\n",
        "\n",
        "# Convert F1 scores to a DataFrame\n",
        "f1_df = pd.DataFrame(f1_scores, columns=np.unique(y_test))\n",
        "\n",
        "# Melt the DataFrame for seaborn compatibility\n",
        "f1_melted = f1_df.melt(var_name='Class', value_name='F1 Score')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='Class', y='F1 Score', data=f1_melted)\n",
        "plt.title('F1 Score Distribution by Class Across Folds')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:09.543618Z",
          "iopub.execute_input": "2024-02-10T10:17:09.544005Z",
          "iopub.status.idle": "2024-02-10T10:17:11.067893Z",
          "shell.execute_reply.started": "2024-02-10T10:17:09.543974Z",
          "shell.execute_reply": "2024-02-10T10:17:11.066548Z"
        },
        "trusted": true,
        "id": "pct1aHU6CybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- class 0 (detractors) shows very robust perfromance with a high f1 score and it frequently shows the high f1 score which is a good sign.\n",
        "- class 1 (passives/neutrals) shows more variability in performance with the lowest performance being 96.5% which is still not too bad. This is due to the fact that there are very few samples in this class.\n",
        "- class 2 (promoters) shows good performance as well, having lesser variability than class 1 but not as high of performance as class 0. This is stil fine since it is more important to focus on the detractors rather than the promoters as negative word of mouth is far worse and long lasting."
      ],
      "metadata": {
        "id": "sTOGskzkCybQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score, matthews_corrcoef\n",
        "\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa: {kappa}\")\n",
        "\n",
        "matthews_corr = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthew's Correlation Coefficient: {matthews_corr}\")"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:11.069655Z",
          "iopub.execute_input": "2024-02-10T10:17:11.070036Z",
          "iopub.status.idle": "2024-02-10T10:17:11.090008Z",
          "shell.execute_reply.started": "2024-02-10T10:17:11.070005Z",
          "shell.execute_reply": "2024-02-10T10:17:11.088725Z"
        },
        "trusted": true,
        "id": "biiUAmYjCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, valid_scores = learning_curve(clf, X_train_delta_tfidf, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=5)\n",
        "\n",
        "# Calculate mean and std\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
        "valid_scores_std = np.std(valid_scores, axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "plt.title(\"Learning Curves\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:11.091934Z",
          "iopub.execute_input": "2024-02-10T10:17:11.09258Z",
          "iopub.status.idle": "2024-02-10T10:17:18.759042Z",
          "shell.execute_reply.started": "2024-02-10T10:17:11.092531Z",
          "shell.execute_reply": "2024-02-10T10:17:18.757711Z"
        },
        "trusted": true,
        "id": "ZV_debo0CybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_idxs = np.where(y_test != y_pred)[0]\n",
        "for idx in misclassified_idxs[:5]:\n",
        "    print(\"Review Text:\", test_df[\"review_text\"].iloc[idx], end=\"\\n\\n\")\n",
        "    print(\"Clean Text:\", test_df[\"clean_text\"].iloc[idx], end=\"\\n\\n\")\n",
        "    print(f\"True class: {y_test.iloc[idx]}, Predicted class: {y_pred[idx]}\\n\")\n",
        "    print(\"=====================================================================================================================\")"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-10T10:17:18.762671Z",
          "iopub.execute_input": "2024-02-10T10:17:18.763098Z",
          "iopub.status.idle": "2024-02-10T10:17:18.773152Z",
          "shell.execute_reply.started": "2024-02-10T10:17:18.763064Z",
          "shell.execute_reply": "2024-02-10T10:17:18.77202Z"
        },
        "trusted": true,
        "id": "V-VRdC9sCybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASPECT BASED SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "za5eekubCybR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation (topic modelling)"
      ],
      "metadata": {
        "id": "Z2dwGtBCCybR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file_path = \"/kaggle/working/cleaned_dataset_v2.csv\"\n",
        "df = pd.read_csv(data_file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:01.702058Z",
          "iopub.execute_input": "2024-02-10T10:25:01.702566Z",
          "iopub.status.idle": "2024-02-10T10:25:03.020454Z",
          "shell.execute_reply.started": "2024-02-10T10:25:01.702524Z",
          "shell.execute_reply": "2024-02-10T10:25:03.019065Z"
        },
        "trusted": true,
        "id": "sfSJHnD1CybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df[df['clean_text'] == ''].index, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "reviews = df['clean_text'].apply(lambda x: x.split())\n",
        "print(\"Length is\", len(reviews), end=\"\\n\\n\")\n",
        "pprint(reviews[:5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:03.022332Z",
          "iopub.execute_input": "2024-02-10T10:25:03.022722Z",
          "iopub.status.idle": "2024-02-10T10:25:03.491156Z",
          "shell.execute_reply.started": "2024-02-10T10:25:03.022687Z",
          "shell.execute_reply": "2024-02-10T10:25:03.489946Z"
        },
        "trusted": true,
        "id": "H5DTxa-dCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(reviews)\n",
        "corpus = [id2word.doc2bow(i) for i in reviews]\n",
        "corpus[0] # looking at first entry"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:05.313025Z",
          "iopub.execute_input": "2024-02-10T10:25:05.314302Z",
          "iopub.status.idle": "2024-02-10T10:25:12.500604Z",
          "shell.execute_reply.started": "2024-02-10T10:25:05.314255Z",
          "shell.execute_reply": "2024-02-10T10:25:12.498959Z"
        },
        "trusted": true,
        "id": "YZy2-RmnCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_example = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]\n",
        "corpus_example[0][:10] # printing 10 words from corpus"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:12.502638Z",
          "iopub.execute_input": "2024-02-10T10:25:12.50386Z",
          "iopub.status.idle": "2024-02-10T10:25:12.527666Z",
          "shell.execute_reply.started": "2024-02-10T10:25:12.503803Z",
          "shell.execute_reply": "2024-02-10T10:25:12.526359Z"
        },
        "trusted": true,
        "id": "stfD3NqTCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Models (topic modelling)"
      ],
      "metadata": {
        "id": "IjfYn3MCCybR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 10\n",
        "\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=id2word,\n",
        "                     num_topics=num_topics, iterations=400, alpha='symmetric', per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:12.529747Z",
          "iopub.execute_input": "2024-02-10T10:25:12.531589Z",
          "iopub.status.idle": "2024-02-10T10:25:58.857875Z",
          "shell.execute_reply.started": "2024-02-10T10:25:12.531531Z",
          "shell.execute_reply": "2024-02-10T10:25:58.856782Z"
        },
        "trusted": true,
        "id": "0DT2PHqGCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Hyperparameter Tuning (topic modelling)"
      ],
      "metadata": {
        "id": "nGqnKtEUCybR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:25:58.860865Z",
          "iopub.execute_input": "2024-02-10T10:25:58.861471Z",
          "iopub.status.idle": "2024-02-10T10:26:53.354713Z",
          "shell.execute_reply.started": "2024-02-10T10:25:58.861434Z",
          "shell.execute_reply": "2024-02-10T10:26:53.353304Z"
        },
        "trusted": true,
        "id": "_nTULQveCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "# Perplexity\n",
        "print('\\nPerplexity : ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "# Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:26:53.356888Z",
          "iopub.execute_input": "2024-02-10T10:26:53.358319Z",
          "iopub.status.idle": "2024-02-10T10:28:04.033338Z",
          "shell.execute_reply.started": "2024-02-10T10:26:53.358264Z",
          "shell.execute_reply": "2024-02-10T10:28:04.03105Z"
        },
        "trusted": true,
        "id": "x0DseTblCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(reviews, min_count=5, threshold=70) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[reviews], threshold=70)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "bigram_reviews = make_bigrams(reviews)\n",
        "trigram_reviews = make_trigrams(reviews)\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(trigram_reviews)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in trigram_reviews]\n",
        "\n",
        "# Build LDA model\n",
        "num_topics = 6\n",
        "\n",
        "lda_model = LdaMulticore(corpus=corpus,\n",
        "                         id2word=id2word,\n",
        "                         num_topics=num_topics,\n",
        "                         iterations=400,\n",
        "                         eta=0.01,\n",
        "                         alpha='symmetric',\n",
        "                         per_word_topics=True)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T11:09:22.811235Z",
          "iopub.execute_input": "2024-02-10T11:09:22.811783Z",
          "iopub.status.idle": "2024-02-10T11:11:14.399232Z",
          "shell.execute_reply.started": "2024-02-10T11:09:22.811733Z",
          "shell.execute_reply": "2024-02-10T11:11:14.39733Z"
        },
        "trusted": true,
        "id": "6Kq1aaENCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T11:11:14.401755Z",
          "iopub.execute_input": "2024-02-10T11:11:14.402118Z",
          "iopub.status.idle": "2024-02-10T11:12:47.496472Z",
          "shell.execute_reply.started": "2024-02-10T11:11:14.402084Z",
          "shell.execute_reply": "2024-02-10T11:12:47.494784Z"
        },
        "trusted": true,
        "id": "Ijuzdeh7CybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Set parameters for coherence values computation\n",
        "limit = 40; start = 2; step = 6;\n",
        "\n",
        "# Call the function\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=trigram_reviews, start=start, limit=limit, step=step)\n",
        "\n",
        "# Show graph\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
        "\n",
        "# Select the model and print the topics\n",
        "optimal_model = model_list[coherence_values.index(max(coherence_values))]\n",
        "model_topics = optimal_model.show_topics(formatted=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T12:38:19.455423Z",
          "iopub.execute_input": "2024-02-10T12:38:19.455807Z"
        },
        "trusted": true,
        "id": "uG_TOW_vCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "# Perplexity\n",
        "print('\\nPerplexity : ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "# Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T11:04:21.058182Z",
          "iopub.execute_input": "2024-02-10T11:04:21.05867Z",
          "iopub.status.idle": "2024-02-10T11:06:11.823811Z",
          "shell.execute_reply.started": "2024-02-10T11:04:21.058622Z",
          "shell.execute_reply": "2024-02-10T11:06:11.822417Z"
        },
        "trusted": true,
        "id": "lqtwSvjQCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "\n",
        "space = {\n",
        "    'n_topics': hp.choice('n_topics', range(2, 12, 2)),\n",
        "    'alpha': hp.choice('alpha', ['symmetric', 'asymmetric', 0.01, 0.1, 0.5, 1.0]),\n",
        "    'eta': hp.choice('eta', ['auto', 0.01, 0.1, 0.5, 1.0])\n",
        "}\n",
        "\n",
        "def objective(params):\n",
        "    lda_model = LdaMulticore(corpus=corpus,\n",
        "                             id2word=id2word,\n",
        "                             num_topics=params['n_topics'],\n",
        "                             alpha=params['alpha'],\n",
        "                             eta=params['eta'],\n",
        "                             random_state=42,\n",
        "                             chunksize=100,\n",
        "                             passes=10,\n",
        "                             per_word_topics=True)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews, dictionary=id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "    return {'loss': -coherence_lda, 'status': STATUS_OK}\n",
        "\n",
        "max_evals = 20\n",
        "early_stopping_rounds = 3\n",
        "best_score = None\n",
        "no_improvement_count = 0\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "for i in range(1, max_evals + 1):\n",
        "    best = fmin(fn=objective,\n",
        "                space=space,\n",
        "                algo=tpe.suggest,\n",
        "                max_evals=i,\n",
        "                trials=trials)\n",
        "\n",
        "    current_best_score = -trials.best_trial['result']['loss']\n",
        "    if best_score is None or current_best_score > best_score:\n",
        "        best_score = current_best_score\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= early_stopping_rounds:\n",
        "        print(f\"Early stopping triggered after {i} evaluations.\")\n",
        "        break\n",
        "\n",
        "print(\"Best parameters:\", best)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T10:30:08.952327Z",
          "iopub.execute_input": "2024-02-10T10:30:08.952812Z",
          "iopub.status.idle": "2024-02-10T10:36:50.201804Z",
          "shell.execute_reply.started": "2024-02-10T10:30:08.952771Z",
          "shell.execute_reply": "2024-02-10T10:36:50.199031Z"
        },
        "trusted": true,
        "id": "1F6BCdAgCybR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning the best parameters are:\n",
        "\n",
        "Alpha (document-topic density): 'symmetric'\n",
        "Eta (word-topic density): 0.01\n",
        "Number of Topics: 2\n",
        "\n",
        "(the values given out after tuning are the index)"
      ],
      "metadata": {
        "id": "NcqZ3hivCybS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus=corpus,\n",
        "                         id2word=id2word,\n",
        "                         num_topics=2,  # from n_topics\n",
        "                         alpha='symmetric',  # from alpha\n",
        "                         eta=0.01,  # from eta\n",
        "                         random_state=42,\n",
        "                         chunksize=100,\n",
        "                         passes=10,\n",
        "                         per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T08:40:10.607025Z",
          "iopub.status.idle": "2024-02-10T08:40:10.607575Z",
          "shell.execute_reply.started": "2024-02-10T08:40:10.607307Z",
          "shell.execute_reply": "2024-02-10T08:40:10.607328Z"
        },
        "trusted": true,
        "id": "2LmC9MQeCybS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "# Perplexity\n",
        "print('\\nPerplexity : ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "# Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-10T08:40:10.609219Z",
          "iopub.status.idle": "2024-02-10T08:40:10.609749Z",
          "shell.execute_reply.started": "2024-02-10T08:40:10.609474Z",
          "shell.execute_reply": "2024-02-10T08:40:10.609496Z"
        },
        "trusted": true,
        "id": "ZuGAlupgCybS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model"
      ],
      "metadata": {
        "id": "zy99Oi0GCybS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "g3xvUBXcCybS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EaKrnbakCybS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}